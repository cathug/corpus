{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYCANTONESE_PATH = r'/home/lun/csrp/corpuses/pycantonese/'\n",
    "CORPUS_PATH = r'/home/lun/csrp/corpuses/hkcancor/'\n",
    "OUTPUT_PATH = r'/home/lun/csrp/jieba/jieba/'\n",
    "\n",
    "import sys, re, glob, math, collections\n",
    "sys.path.insert(0, PYCANTONESE_PATH)\n",
    "import pycantonese as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text and pos tags from HKCanCor CHAT files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def replaceCantoneseCharacters(text):\n",
    "    reg_exp_dict = {\n",
    "        '':'㗎', \n",
    "#         '':'𠺢',\n",
    "        '':'噃', '':'𠺝', '':'喎',\n",
    "#         '':'尛'\n",
    "        '':'liu', '':'lung', '':'𧕴',\n",
    "        '':'喵', '\\?幾':'幾', '':'𥅾',\n",
    "#         '':'zong',\n",
    "        '':'嚕',\n",
    "#         '':'lu',\n",
    "        '':'呢', \n",
    "#         '':'le', \n",
    "        '':'𥄫',\n",
    "        \n",
    "        # also replace with empty character \n",
    "        # if sentence ends with/contains .!。…?？\n",
    "        ',|\\\"|-|\\.|\\!|。|…|\\?|？':''\n",
    "    }\n",
    "        \n",
    "    for replace_char, sub_char in reg_exp_dict.items():\n",
    "        text = re.sub(replace_char, sub_char, text, count=0, flags=re.DOTALL)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSymbols(string):\n",
    "    return re.sub('-|,|\\.|\\?|\\\"|\\!', '', string, count=0, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to do batch extracting text from HK Cantonese corpus data \n",
    "# i.e. the 58 annotated transcripts in http://compling.hss.ntu.edu.sg/hkcancor/\n",
    "# that are reproduced in CHAT format in PyCantonese data folder\n",
    "# Idea is to keep extracted data in separate files for flexibility\n",
    "# This means something like corpus = pc.hkcancor() cannot be called\n",
    "def save_transcriptAndPOSTags():\n",
    "    chat_files = glob.glob(PYCANTONESE_PATH + 'pycantonese/data/hkcancor/*.cha')\n",
    "    \n",
    "    # a helper to save each item in hkcorpus_postag and \n",
    "    # hkcorpus_text in its own line\n",
    "    def saveFile(str_list, filename):\n",
    "        with open(CORPUS_PATH + filename % i, 'w', encoding='utf-8') as f:\n",
    "            str_list = map(lambda x: x+\"\\n\", str_list)\n",
    "            f.writelines(str_list)\n",
    "        assert(f.closed)\n",
    "    \n",
    "    \n",
    "    for i, chat_file in enumerate(chat_files):\n",
    "        corpus = pc.read_chat(chat_file, encoding='utf-8')\n",
    "\n",
    "        # word_sents() and pos_sents() called in the following\n",
    "        # lines are customized methods in the CantoneseCHATReader \n",
    "        # class.  They are not in the official pyCantonese library\n",
    "\n",
    "        \n",
    "        # join strings in each list,\n",
    "        # replace Cantonese characters and remove leading whitespaces \n",
    "        hkcorpus_text = [replaceCantoneseCharacters(\n",
    "            \" \".join(x)).strip(' ') for x in corpus.word_sents()]\n",
    "\n",
    "    #    # find files with wrongly decoded Cantonese characters\n",
    "    #     findword = re.search('|', hkcorpus_text, flags=0)\n",
    "    #     if findword:\n",
    "    #         print(chat_file)\n",
    "\n",
    "        # join strings in each list, replace symbols, \n",
    "        # remove leading whitespaces, and change pos to lower case\n",
    "        hkcorpus_postag = [removeSymbols(\n",
    "            \" \".join(x)).strip(' ').lower() for x in corpus.pos_sents()]\n",
    "        \n",
    "    \n",
    "        saveFile(hkcorpus_text, r'text/hk_cantonese_corpus_%d.txt')\n",
    "        saveFile(hkcorpus_postag, r'pos/hk_cantonese_corpus_pos_%d.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_transcriptAndPOSTags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile HMM data from output in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 58\n",
    "dataframes = []\n",
    "for i in range(corpus_size):\n",
    "    with open(CORPUS_PATH + r'text/hk_cantonese_corpus_%d.txt' % i, \n",
    "        'r', encoding='utf-8') as ftext:\n",
    "        \n",
    "        # put each string in list into a list of words,\n",
    "        # removing all empty entries in latter\n",
    "        text_list = [x.split(\" \") for x in ftext.read().splitlines()]\n",
    "        text_list = [list(filter(str.strip, x )) for x in text_list]\n",
    "    assert(ftext.closed)\n",
    "    \n",
    "    \n",
    "    with open(CORPUS_PATH + r'pos/hk_cantonese_corpus_pos_%d.txt' % i,\n",
    "        'r', encoding='utf-8') as fpos:\n",
    "        \n",
    "        # put each string in list into a list of pos tags\n",
    "        # removing all empty entries in latter\n",
    "        pos_list = [x.split(\" \") for x in fpos.read().splitlines()]\n",
    "        pos_list = [list(filter(str.strip, x)) for x in pos_list]\n",
    "    assert(fpos.closed)\n",
    "    \n",
    "    \n",
    "    table = pd.DataFrame({ 'text' : text_list, 'pos' : pos_list })\n",
    "    dataframes.append(table)\n",
    "    \n",
    "df = pd.concat(dataframes, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'pos']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[有, 有, 一, 間, 呢, 就, 賣, 開, 誒, 家庭, 用品, 嘅, 就, 即係, ...</td>\n",
       "      <td>[v1, v1, m, q, y1, d, v, u, e, n, n, y, d, c, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[就, 爭取, 到, 六成, 嘅, 減, 租, 啊]</td>\n",
       "      <td>[d, v, u, m, u, v, ng, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[噉樣, 就, 即係, 令到, 個, 商鋪, 呢, 打, 咗, 支, 強心針, 就, 堅持,...</td>\n",
       "      <td>[c, d, c, v, q, n, y1, v, u, q, n, d, v, d, d,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[重, 有得, 搞]</td>\n",
       "      <td>[d, vu, v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[喀]</td>\n",
       "      <td>[e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[但係, 其實, 而家, 已經, 變, 咗, 攻防戰, 𡃉, 嚹]</td>\n",
       "      <td>[c, d, t, d, v, u, n, y, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[即, 係, 你有過牆梯, 唔係, 你有張良計, 我有過牆梯, 𡃉, 嘞]</td>\n",
       "      <td>[c, v, l, v, l, l, y, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[你有過牆梯]</td>\n",
       "      <td>[l]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[噉, 𠻺, 點, 呢]</td>\n",
       "      <td>[c, d, r, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[我, 就, 出, 多, 自己, 一, 條, 張良計, 噉樣, 啊, 喀]</td>\n",
       "      <td>[r, d, v, a, r, m, q, l, c, y, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[哦]</td>\n",
       "      <td>[e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[我, 有, 禽流雞]</td>\n",
       "      <td>[r, v1, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[係, 嘞]</td>\n",
       "      <td>[v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[噉, 個, 情況, 點, 呢]</td>\n",
       "      <td>[c, q, n, r, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[就, 而家, 呢, 即, 係, 譬如, 而家, 出, 好多, 招數, 𡃉, 嗰啲, 商舖, 喇]</td>\n",
       "      <td>[d, t, y1, c, v, v, t, v, m, n, y, r, n, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[第一, 最, 簡單, 嘅, 就, 扮, 可憐, 先]</td>\n",
       "      <td>[m, d, a, u, d, v, a, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[即係, 同, 人哋, 講, 喂, 我哋, 好, 慘, 啊]</td>\n",
       "      <td>[c, p, r, v, e, r, d, a, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[好, 淒涼]</td>\n",
       "      <td>[d, a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[都, 其實, 都, 唔使, 扮]</td>\n",
       "      <td>[d, d, d, vu, v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[上去, 想, 囈, 喇]</td>\n",
       "      <td>[v, vu, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[你, 行, 出去, 望, 下]</td>\n",
       "      <td>[r, v, v, v, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[影, 啲, 相, 啊, 噉樣]</td>\n",
       "      <td>[v, q, n, y, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[你, 淨係, 叫, 業主, 來, 吖]</td>\n",
       "      <td>[r, d, v, n, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[係, 囖]</td>\n",
       "      <td>[v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[喂, 但係, 業主, 唔, 理, 你, 㗎, 嗎]</td>\n",
       "      <td>[e, c, n, d, v, r, y, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[坐, 喺, office, 裏邊, 靚, 靚, 哋, 噉樣]</td>\n",
       "      <td>[v, p, xn, f, a, a, u, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[唔係, 噉, 你, 扮, 可憐, 都, 要, 㗎, 嗎]</td>\n",
       "      <td>[v, c, r, v, a, d, vu, y, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[係, 啊]</td>\n",
       "      <td>[v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[一係, 話, 俾, 佢, 聽, 點, 可憐, 法, 囖]</td>\n",
       "      <td>[d, v, p, r, v, r, a, n, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[你, 咪, 拍, 嗰度, 情形, 俾, 佢, 睇, 囖]</td>\n",
       "      <td>[r, d, v, r, n, p, r, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15852</th>\n",
       "      <td>[我, 都, 唔, 知, 佢, 講, 𡁵, 乜]</td>\n",
       "      <td>[r, d, d, v, r, v, u, r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15853</th>\n",
       "      <td>[死, 喇]</td>\n",
       "      <td>[v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15854</th>\n",
       "      <td>[嘩, 你, 隻, 手, 已經, 搲, 到, 死, 喇, 有, 一, 笪, 蚊𧕴, 嚹]</td>\n",
       "      <td>[e, r, q, n, d, v, p, v, y, v1, m, q, n, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15855</th>\n",
       "      <td>[我, 都, 係, 啊]</td>\n",
       "      <td>[r, d, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15856</th>\n",
       "      <td>[好, cheap, 啊, 哩度]</td>\n",
       "      <td>[d, xa, y, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15857</th>\n",
       "      <td>[要, 唔, 要, 走]</td>\n",
       "      <td>[vu, d, vu, v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15858</th>\n",
       "      <td>[不過, 你, 要, 等, 錄音]</td>\n",
       "      <td>[c, r, vu, v, v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15859</th>\n",
       "      <td>[講, 完, 未, 啊]</td>\n",
       "      <td>[v, u, d, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15860</th>\n",
       "      <td>[ei1, 你, 想, 繼續, 講, 我, 重, 可以, 講, 𠸏]</td>\n",
       "      <td>[e, r, vu, v, v, r, d, vu, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15861</th>\n",
       "      <td>[我, 想, 去, 歐洲, 啊]</td>\n",
       "      <td>[r, vu, v, ns, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15862</th>\n",
       "      <td>[我, 想, 去, 英國, 喇, 誒, 德國, 喇, 丹麥, 喇, 誒, 同埋, 上邊, 嗰...</td>\n",
       "      <td>[r, vu, v, ns, y1, e, ns, y1, ns, y1, e, c, f,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15863</th>\n",
       "      <td>[瑞典]</td>\n",
       "      <td>[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15864</th>\n",
       "      <td>[瑞典, 法國]</td>\n",
       "      <td>[ns, ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15865</th>\n",
       "      <td>[即係, 上邊, 嗰, 幾, 嗰, 拃]</td>\n",
       "      <td>[c, f, r, m, r, q]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15866</th>\n",
       "      <td>[嗰, 拃, 話, 好, 好, 吖, 嗎]</td>\n",
       "      <td>[r, q, v, d, a, y, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15867</th>\n",
       "      <td>[嗰, 拃, 啲, 人, 啲, 英文, 好, 好]</td>\n",
       "      <td>[r, q, q, n, q, nz, d, a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15868</th>\n",
       "      <td>[瑞典]</td>\n",
       "      <td>[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15869</th>\n",
       "      <td>[係, 啊]</td>\n",
       "      <td>[v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15870</th>\n",
       "      <td>[話, 北歐, 啲, 人, 啲, 英文, 好, 好]</td>\n",
       "      <td>[v, ns, q, n, q, nz, d, a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15871</th>\n",
       "      <td>[好, 遠, 啊, 不過, 好似]</td>\n",
       "      <td>[d, a, y, c, v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15872</th>\n",
       "      <td>[會, 好, 遠, 𡃉, 呵]</td>\n",
       "      <td>[vu, d, a, y, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15873</th>\n",
       "      <td>[幾, 喇]</td>\n",
       "      <td>[d, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15874</th>\n",
       "      <td>[但係, 真係, 如果, 去, 嗰度, 即係, 梗係, 去, 一, 個, at, least...</td>\n",
       "      <td>[c, d, c, v, r, c, d, v, m, q, xp, xd, v, m, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>[即係, 其實, 你, 就噉, 放, 兩, 個, 禮拜, 都, 得, 𠸏]</td>\n",
       "      <td>[c, d, r, d, v, m, q, n, d, vu, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15876</th>\n",
       "      <td>[噉, 去, 一, 去, 英國, 返來]</td>\n",
       "      <td>[c, v, m, v, ns, f]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>[噉, 如果, 你, 唔, 嫌, 張, 機票, 嘥, 都, 冇所謂, 𠸏]</td>\n",
       "      <td>[c, c, r, d, v, q, n, v, d, v, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>[其實, 張, 機票, 都, 唔係, 好, 貴, 啫]</td>\n",
       "      <td>[d, q, n, d, v, d, a, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15879</th>\n",
       "      <td>[五千幾]</td>\n",
       "      <td>[m]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880</th>\n",
       "      <td>[噉, 你, 去, 日本, 都, 二, 三千, 喇, at, least, 而家]</td>\n",
       "      <td>[c, r, v, ns, d, m, m, y, xp, xd, t]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15881</th>\n",
       "      <td>[但係, 啲, 人, 通常, 呢, 都, 係, 放, 誒, 誒, 兩, 個, 禮拜, 就, ...</td>\n",
       "      <td>[c, q, n, d, y1, d, v, v, e, e, m, q, n, d, v,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      [有, 有, 一, 間, 呢, 就, 賣, 開, 誒, 家庭, 用品, 嘅, 就, 即係, ...   \n",
       "1                             [就, 爭取, 到, 六成, 嘅, 減, 租, 啊]   \n",
       "2      [噉樣, 就, 即係, 令到, 個, 商鋪, 呢, 打, 咗, 支, 強心針, 就, 堅持,...   \n",
       "3                                             [重, 有得, 搞]   \n",
       "4                                                    [喀]   \n",
       "5                      [但係, 其實, 而家, 已經, 變, 咗, 攻防戰, 𡃉, 嚹]   \n",
       "6                  [即, 係, 你有過牆梯, 唔係, 你有張良計, 我有過牆梯, 𡃉, 嘞]   \n",
       "7                                                [你有過牆梯]   \n",
       "8                                           [噉, 𠻺, 點, 呢]   \n",
       "9                  [我, 就, 出, 多, 自己, 一, 條, 張良計, 噉樣, 啊, 喀]   \n",
       "10                                                   [哦]   \n",
       "11                                           [我, 有, 禽流雞]   \n",
       "12                                                [係, 嘞]   \n",
       "13                                      [噉, 個, 情況, 點, 呢]   \n",
       "14     [就, 而家, 呢, 即, 係, 譬如, 而家, 出, 好多, 招數, 𡃉, 嗰啲, 商舖, 喇]   \n",
       "15                           [第一, 最, 簡單, 嘅, 就, 扮, 可憐, 先]   \n",
       "16                        [即係, 同, 人哋, 講, 喂, 我哋, 好, 慘, 啊]   \n",
       "17                                               [好, 淒涼]   \n",
       "18                                     [都, 其實, 都, 唔使, 扮]   \n",
       "19                                         [上去, 想, 囈, 喇]   \n",
       "20                                      [你, 行, 出去, 望, 下]   \n",
       "21                                      [影, 啲, 相, 啊, 噉樣]   \n",
       "22                                  [你, 淨係, 叫, 業主, 來, 吖]   \n",
       "23                                                [係, 囖]   \n",
       "24                            [喂, 但係, 業主, 唔, 理, 你, 㗎, 嗎]   \n",
       "25                       [坐, 喺, office, 裏邊, 靚, 靚, 哋, 噉樣]   \n",
       "26                         [唔係, 噉, 你, 扮, 可憐, 都, 要, 㗎, 嗎]   \n",
       "27                                                [係, 啊]   \n",
       "28                         [一係, 話, 俾, 佢, 聽, 點, 可憐, 法, 囖]   \n",
       "29                         [你, 咪, 拍, 嗰度, 情形, 俾, 佢, 睇, 囖]   \n",
       "...                                                  ...   \n",
       "15852                           [我, 都, 唔, 知, 佢, 講, 𡁵, 乜]   \n",
       "15853                                             [死, 喇]   \n",
       "15854       [嘩, 你, 隻, 手, 已經, 搲, 到, 死, 喇, 有, 一, 笪, 蚊𧕴, 嚹]   \n",
       "15855                                       [我, 都, 係, 啊]   \n",
       "15856                                  [好, cheap, 啊, 哩度]   \n",
       "15857                                       [要, 唔, 要, 走]   \n",
       "15858                                  [不過, 你, 要, 等, 錄音]   \n",
       "15859                                       [講, 完, 未, 啊]   \n",
       "15860                 [ei1, 你, 想, 繼續, 講, 我, 重, 可以, 講, 𠸏]   \n",
       "15861                                   [我, 想, 去, 歐洲, 啊]   \n",
       "15862  [我, 想, 去, 英國, 喇, 誒, 德國, 喇, 丹麥, 喇, 誒, 同埋, 上邊, 嗰...   \n",
       "15863                                               [瑞典]   \n",
       "15864                                           [瑞典, 法國]   \n",
       "15865                               [即係, 上邊, 嗰, 幾, 嗰, 拃]   \n",
       "15866                              [嗰, 拃, 話, 好, 好, 吖, 嗎]   \n",
       "15867                          [嗰, 拃, 啲, 人, 啲, 英文, 好, 好]   \n",
       "15868                                               [瑞典]   \n",
       "15869                                             [係, 啊]   \n",
       "15870                         [話, 北歐, 啲, 人, 啲, 英文, 好, 好]   \n",
       "15871                                  [好, 遠, 啊, 不過, 好似]   \n",
       "15872                                    [會, 好, 遠, 𡃉, 呵]   \n",
       "15873                                             [幾, 喇]   \n",
       "15874  [但係, 真係, 如果, 去, 嗰度, 即係, 梗係, 去, 一, 個, at, least...   \n",
       "15875              [即係, 其實, 你, 就噉, 放, 兩, 個, 禮拜, 都, 得, 𠸏]   \n",
       "15876                               [噉, 去, 一, 去, 英國, 返來]   \n",
       "15877              [噉, 如果, 你, 唔, 嫌, 張, 機票, 嘥, 都, 冇所謂, 𠸏]   \n",
       "15878                        [其實, 張, 機票, 都, 唔係, 好, 貴, 啫]   \n",
       "15879                                              [五千幾]   \n",
       "15880          [噉, 你, 去, 日本, 都, 二, 三千, 喇, at, least, 而家]   \n",
       "15881  [但係, 啲, 人, 通常, 呢, 都, 係, 放, 誒, 誒, 兩, 個, 禮拜, 就, ...   \n",
       "\n",
       "                                                     pos  \n",
       "0      [v1, v1, m, q, y1, d, v, u, e, n, n, y, d, c, ...  \n",
       "1                              [d, v, u, m, u, v, ng, y]  \n",
       "2      [c, d, c, v, q, n, y1, v, u, q, n, d, v, d, d,...  \n",
       "3                                             [d, vu, v]  \n",
       "4                                                    [e]  \n",
       "5                            [c, d, t, d, v, u, n, y, y]  \n",
       "6                               [c, v, l, v, l, l, y, y]  \n",
       "7                                                    [l]  \n",
       "8                                           [c, d, r, y]  \n",
       "9                      [r, d, v, a, r, m, q, l, c, y, e]  \n",
       "10                                                   [e]  \n",
       "11                                            [r, v1, n]  \n",
       "12                                                [v, y]  \n",
       "13                                       [c, q, n, r, y]  \n",
       "14           [d, t, y1, c, v, v, t, v, m, n, y, r, n, y]  \n",
       "15                              [m, d, a, u, d, v, a, d]  \n",
       "16                           [c, p, r, v, e, r, d, a, y]  \n",
       "17                                                [d, a]  \n",
       "18                                      [d, d, d, vu, v]  \n",
       "19                                         [v, vu, v, y]  \n",
       "20                                       [r, v, v, v, u]  \n",
       "21                                       [v, q, n, y, c]  \n",
       "22                                    [r, d, v, n, v, y]  \n",
       "23                                                [v, y]  \n",
       "24                              [e, c, n, d, v, r, y, y]  \n",
       "25                             [v, p, xn, f, a, a, u, c]  \n",
       "26                          [v, c, r, v, a, d, vu, y, y]  \n",
       "27                                                [v, y]  \n",
       "28                           [d, v, p, r, v, r, a, n, y]  \n",
       "29                           [r, d, v, r, n, p, r, v, y]  \n",
       "...                                                  ...  \n",
       "15852                           [r, d, d, v, r, v, u, r]  \n",
       "15853                                             [v, y]  \n",
       "15854        [e, r, q, n, d, v, p, v, y, v1, m, q, n, y]  \n",
       "15855                                       [r, d, v, y]  \n",
       "15856                                      [d, xa, y, n]  \n",
       "15857                                     [vu, d, vu, v]  \n",
       "15858                                   [c, r, vu, v, v]  \n",
       "15859                                       [v, u, d, y]  \n",
       "15860                   [e, r, vu, v, v, r, d, vu, v, y]  \n",
       "15861                                  [r, vu, v, ns, y]  \n",
       "15862  [r, vu, v, ns, y1, e, ns, y1, ns, y1, e, c, f,...  \n",
       "15863                                               [ns]  \n",
       "15864                                           [ns, ns]  \n",
       "15865                                 [c, f, r, m, r, q]  \n",
       "15866                              [r, q, v, d, a, y, y]  \n",
       "15867                          [r, q, q, n, q, nz, d, a]  \n",
       "15868                                               [ns]  \n",
       "15869                                             [v, y]  \n",
       "15870                         [v, ns, q, n, q, nz, d, a]  \n",
       "15871                                    [d, a, y, c, v]  \n",
       "15872                                   [vu, d, a, y, e]  \n",
       "15873                                             [d, y]  \n",
       "15874  [c, d, c, v, r, c, d, v, m, q, xp, xd, v, m, q...  \n",
       "15875                 [c, d, r, d, v, m, q, n, d, vu, y]  \n",
       "15876                                [c, v, m, v, ns, f]  \n",
       "15877                  [c, c, r, d, v, q, n, v, d, v, y]  \n",
       "15878                           [d, q, n, d, v, d, a, y]  \n",
       "15879                                                [m]  \n",
       "15880               [c, r, v, ns, d, m, m, y, xp, xd, t]  \n",
       "15881  [c, q, n, d, y1, d, v, v, e, e, m, q, n, d, v,...  \n",
       "\n",
       "[15882 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if string is an ascii\n",
    "def string_is_ascii(string):\n",
    "    try:\n",
    "        string.encode(encoding='ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# preprocessing function for BMES tagging of words\n",
    "#\n",
    "# It separates words to a list of characters\n",
    "# To preserve the ascii word,\n",
    "# find the first pointer ascii character position\n",
    "# find the last pointer ascii character position\n",
    "# concatenate ascii characters in the sublist, \n",
    "# removing empty strings and white spaces in between\n",
    "# parameter: word - word\n",
    "# returns: a list of tokens\n",
    "def tokenize_word(word):\n",
    "    char_list = list(word)\n",
    "#     print(\"Word is separated to : \" + str(char_list))\n",
    "\n",
    "    first_ascii_pos = []\n",
    "    last_ascii_pos = []\n",
    "    \n",
    "    ascii_flag = False\n",
    "    for i, c in enumerate(char_list):\n",
    "#         print(\"The current character is %s\" %c)\n",
    "        if string_is_ascii(c):           \n",
    "            if ascii_flag == False:\n",
    "#                 print(\"ascii set to true\")\n",
    "                first_ascii_pos.append(i)\n",
    "                ascii_flag = True\n",
    "            if i == len(char_list) - 1:\n",
    "                last_ascii_pos.append(len(char_list))\n",
    "        else:\n",
    "            if ascii_flag == True:\n",
    "#                 print(\"ascii set to false\")\n",
    "                last_ascii_pos.append(i)\n",
    "                ascii_flag = False\n",
    "        \n",
    "    if len(first_ascii_pos): # if array is not empty\n",
    "#         print(first_ascii_pos[::-1])\n",
    "#         print(last_ascii_pos[::-1])\n",
    "        for i, j in zip(first_ascii_pos[::-1], last_ascii_pos[::-1]):\n",
    "#             print(i, j)\n",
    "            char_list[i:j] = list(\n",
    "                filter(None, \"\".join(char_list[i:j]).split(\" \") ) )\n",
    "    return char_list\n",
    "\n",
    "    \n",
    "\n",
    "# function to tag words using the\n",
    "# BMES (begin, middle, end, single) tagging system\n",
    "# precondition: string must not be empty\n",
    "# returns list of separated words and corresponding\n",
    "# BMES tags\n",
    "def tagWord_BMES(word):\n",
    "    word_length = len(word)\n",
    "    assert(word_length)\n",
    "    bmes_list = []\n",
    "    \n",
    "    word_list = tokenize_word(word)\n",
    "    if len(word_list) == 1:\n",
    "        bmes_list.append(\"S\")\n",
    "    else:\n",
    "        for i, w in enumerate(word_list):\n",
    "            if i == 0:\n",
    "                bmes_list.append(\"B\")\n",
    "            elif i == len(word_list) - 1:\n",
    "                bmes_list.append(\"E\")\n",
    "            else:\n",
    "                bmes_list.append(\"M\")\n",
    "        \n",
    "    return bmes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test cases\n",
    "# print(tokenize_word('你office land過牆梯'))\n",
    "# print(tokenize_word('office'))\n",
    "# print(tokenize_word('你office'))\n",
    "# print(tokenize_word('hello過牆梯world'))\n",
    "# print(tokenize_word('hello過牆梯world過牆'))\n",
    "# print(tokenize_word('你過牆梯'))\n",
    "# print(tokenize_word('Hello  World'))\n",
    "\n",
    "# print(tagWord_BMES('你有過牆梯')) # BMMME\n",
    "# print(tagWord_BMES('office')) # S\n",
    "# print(tagWord_BMES('你office')) # BE\n",
    "# print(tagWord_BMES('office牆')) # BE\n",
    "# print(tagWord_BMES('Hello World')) # BE\n",
    "# print(tagWord_BMES('有有')) # BE\n",
    "# print(tagWord_BMES('有')) # S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper to count total number of start instances\n",
    "def countTotalStartInstances(start_dict):\n",
    "    total = 0\n",
    "    for _, value in start_dict.items():\n",
    "        total += value\n",
    "    return total\n",
    "\n",
    "\n",
    "# create training algorithm to calculate \n",
    "# emission (BMES->word) and transition (BMES->BMES)\n",
    "# probabilities\n",
    "# returns: a tuple of prob_trans, prob_emit, and prob_start\n",
    "def trainingHMM_BMESTagging(text_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    \n",
    "    # for prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list in text_lists:\n",
    "        previous = '<s>'\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "        \n",
    "        for j, character in enumerate(line_list):\n",
    "#             print(\"The entry contains %s\" % character)\n",
    "            text_bmesTags_list = tagWord_BMES(character)\n",
    "            if j == 0:\n",
    "                start_tag = text_bmesTags_list[0]\n",
    "                if start_tag not in start:\n",
    "                    start[start_tag] = 0\n",
    "                start[start_tag] += 1\n",
    "        \n",
    "            for i, bmesTag in enumerate(text_bmesTags_list):\n",
    "                \n",
    "                transition_bigram = previous + \" \" + bmesTag\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if bmesTag not in context:\n",
    "                    context[bmesTag] = 0\n",
    "                context[bmesTag] += 1\n",
    "\n",
    "                bigram_emission = bmesTag + \" \" + character[i]\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "\n",
    "                previous = bmesTag\n",
    "            \n",
    "        bigram_transition = previous + \" </s>\"\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "\n",
    "    # output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for key, value in transition.items():\n",
    "        previous_tag, current_tag = key.split(\" \", maxsplit=1)\n",
    "        if previous_tag != '<s>' and current_tag != \"</s>\":\n",
    "            prob_trans[previous_tag][current_tag] = math.log2(float(value)/context[previous_tag])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    \n",
    "    for key, value in emission.items():\n",
    "        tag, word = key.split(\" \", maxsplit=1)\n",
    "#         print(\"Context contains %d instances\" % context[tag])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag])))\n",
    "        prob_emit[tag][word] = math.log2(float(value)/context[tag])\n",
    "    \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag, value in start.items():\n",
    "        prob_start[tag] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    prob_start[\"M\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    prob_start[\"E\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_trans1, prob_emit1, prob_start1 = trainingHMM_BMESTagging(df.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training algorithm to calculate \n",
    "# emission (pos->word) and transition (pos-pos)\n",
    "# probabilities\n",
    "def trainingHMM_POSTagging(text_lists, posTags_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    char_state = {}\n",
    "    \n",
    "    # for char_state_tab.*, prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    pos_tagset1 = [pos_tag for posTag_list in posTags_lists for pos_tag in posTag_list]\n",
    "    pos_tagset2 = ['ag', 'a', 'ad', 'an', 'bg', 'b', 'c', 'dg', 'd', \n",
    "        'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'mg', 'm', 'ng', 'n', \n",
    "        'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'qg', 'q', 'rg', 'r', \n",
    "        's', 'tg', 't', 'ug', 'u', 'vg', 'v', 'vd', 'vn', 'w', 'x', \n",
    "        'yg', 'y', 'z'] # official HKCanCor tagset\n",
    "    unique_pos_tagset = set(pos_tagset1 + pos_tagset2)\n",
    "    bmes_tagset = {'B', 'M', 'E', 'S'}\n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list, linePosTags_list in zip(text_lists, posTags_lists):\n",
    "        previous = ('<s>') # start sentence tag\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "       \n",
    "        for j, (character, pos_tag) in enumerate(\n",
    "            zip(line_list, linePosTags_list) ):\n",
    "#             print(\"The entry contains %s with tag %s \" % (character, pos_tag) )\n",
    "            character_bmesTags_list = tagWord_BMES(character)\n",
    "            character_token_list = tokenize_word(character)\n",
    "            \n",
    "            # build up start dictionary\n",
    "            if j == 0:\n",
    "                start_tag = character_bmesTags_list[0]\n",
    "                if (start_tag, pos_tag) not in start:\n",
    "                    start[(start_tag, pos_tag)] = 0\n",
    "                start[(start_tag, pos_tag)] += 1\n",
    "        \n",
    "            # build up transition, emission dictionaries\n",
    "            for i, (token, bmesTag) in enumerate(\n",
    "                zip(character_token_list, character_bmesTags_list)):\n",
    "                \n",
    "                tag_pair = (bmesTag, pos_tag)\n",
    "                \n",
    "                transition_bigram = (previous, tag_pair)\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if tag_pair not in context:\n",
    "                    context[tag_pair] = 0\n",
    "                context[tag_pair] += 1\n",
    "\n",
    "                bigram_emission = (tag_pair, token)\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "                \n",
    "                if token not in char_state:\n",
    "                    char_state[token] = []\n",
    "                char_state[token].append(tag_pair)\n",
    "                    \n",
    "                \n",
    "                previous = tag_pair\n",
    "            \n",
    "        bigram_transition = (previous, \"</s>\")\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "        \n",
    "    \n",
    "# output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for (previous_tag_pair, current_tag_pair), value in transition.items():\n",
    "        if previous_tag_pair != ('<s>') and current_tag_pair != (\"</s>\"):\n",
    "            prob_trans[previous_tag_pair][current_tag_pair] = math.log2(float(value)/context[previous_tag_pair])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_trans:\n",
    "                prob_trans[(bmes_tag, pos_tag)] = {}\n",
    "                \n",
    "    \n",
    "    for (token, tag_pair_list) in char_state.items():\n",
    "        char_state[token] = tuple(set(tag_pair_list)) # only keep unique tag sets\n",
    "        \n",
    "    \n",
    "    for (tag_pair, word), value in emission.items():\n",
    "#         print(\"Context contains %d instances\" % context[tag_pair])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag_pair])))\n",
    "        prob_emit[tag_pair][word] = math.log2(float(value)/context[tag_pair])\n",
    "    \n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_emit:\n",
    "                prob_emit[(bmes_tag, pos_tag)] = {}\n",
    "\n",
    "                \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag_pair, value in start.items():\n",
    "        prob_start[tag_pair] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_start:\n",
    "                prob_start[(bmes_tag, pos_tag)] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start, char_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_trans2, prob_emit2, prob_start2, char_state2 = trainingHMM_POSTagging(df.text.tolist(), df.pos.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4, s5, s6, s7 = trainingHMM_POSTagging([['重', '有得', '搞']], [['d', 'vu', 'v']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output prob_trans, prob_emit, prob_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputDictionary(filename, prob_dict):\n",
    "    with open(OUTPUT_PATH + filename, 'w', encoding='utf-8') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(\"P=\", end='')\n",
    "            pprint(prob_dict) \n",
    "    assert(f.closed)\n",
    "\n",
    "def pickleDictionary(filename, prob_dict):\n",
    "    with open(OUTPUT_PATH + filename, 'wb') as f:\n",
    "        # jieba uses protocol 0 encoding for its pickle files\n",
    "        pickle.dump(prob_dict, f, protocol=0)\n",
    "    assert(f.closed)\n",
    "    \n",
    "def depickleDictionary(filename):\n",
    "    with open(OUTPUT_PATH + filename, 'rb') as f:\n",
    "        prob_dict = pickle.load(f, encoding='utf-8')\n",
    "    assert(f.closed)\n",
    "    return prob_dict\n",
    "    \n",
    "outputDictionary(\"posseg/prob_trans.py\", prob_trans2)\n",
    "outputDictionary(\"posseg/prob_emit.py\", prob_emit2)\n",
    "outputDictionary(\"posseg/prob_start.py\", prob_start2)\n",
    "outputDictionary(\"posseg/char_state_tab.py\", char_state2)\n",
    "\n",
    "pickleDictionary(\"posseg/prob_trans.p\", prob_trans2)\n",
    "pickleDictionary(\"posseg/prob_emit.p\", prob_emit2)\n",
    "pickleDictionary(\"posseg/prob_start.p\", prob_start2)\n",
    "pickleDictionary(\"posseg/char_state_tab.p\", char_state2)\n",
    "\n",
    "outputDictionary(\"finalseg/prob_trans.py\", prob_trans1)\n",
    "outputDictionary(\"finalseg/prob_emit.py\", prob_emit1)\n",
    "outputDictionary(\"finalseg/prob_start.py\", prob_start1)\n",
    "\n",
    "pickleDictionary(\"finalseg/prob_trans.p\", prob_trans1)\n",
    "pickleDictionary(\"finalseg/prob_emit.p\", prob_emit1)\n",
    "pickleDictionary(\"finalseg/prob_start.p\", prob_start1)\n",
    "\n",
    "\n",
    "# s1 = depickleDictionary(\"finalseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"finalseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"finalseg/prob_start.p\")\n",
    "\n",
    "# s1 = depickleDictionary(\"posseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"posseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"posseg/prob_start.p\")\n",
    "# s4 = depickleDictionary(\"posseg/char_state_tab.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Statistical Data from HKCanCor CHAT files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "corpus = pc.hkcancor()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "words = corpus.search(pos='.+')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def extract_POSInfoFromPyCantonese(words):\n",
    "    word_list = []\n",
    "    for i in range(len(words)):\n",
    "        # we need the word and POS tags only\n",
    "        word_list.append(\" \".join(words[i][0:2]))\n",
    "    return word_list\n",
    "\n",
    "full_list = extract_POSInfoFromPyCantonese(words)\n",
    "\n",
    "print(len(full_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put everything in Pandas\n",
    "# This is not necessary, but \n",
    "# it shows the layouts neatly\n",
    "df_full = pd.DataFrame(full_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full.columns = ['tokens']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full = pd.DataFrame(\n",
    "    df_full.tokens.str.split(' ', 1).tolist(), \n",
    "    columns=['word', 'pos'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full['pos'] = df_full['pos'].str.lower()\n",
    "df_full['word'] = df_full['word'].apply(replaceCantoneseCharacters)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full = df_full.groupby(['word','pos'], sort=False).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# reorder the columns according to Jieba Dictionary layout\n",
    "df_full = df_full[['word', 'count', 'pos']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove all ?.\"'! etc\n",
    "df_full.drop(df_full.index[:7], inplace=True)\n",
    "df_full.drop(df_full.index[2], inplace=True)\n",
    "df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_nouns = df_full[df_full.pos.str.contains('^n|[^va]n', regex=True)]\n",
    "df_nouns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_others = df_full[~df_full.isin(df_nouns)].dropna()\n",
    "df_others"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_nouns.word.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/nouns.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_others.word.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/others.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/hkcantonesedict.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
