{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, glob, math, collections, os\n",
    "# sys.path.insert(0, PYCANTONESE_PATH) # PyCantonese v0.21, modified\n",
    "# import pycantonese as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "# PYCANTONESE_PATH = os.path.expanduser(r'~/csrp/corpus/pycantonese/')\n",
    "CORPUS_PATH = os.path.expanduser('~/csrp/corpus/hkcancor/')\n",
    "OUTPUT_PATH = os.path.expanduser('~/csrp/jieba-cantonese/')\n",
    "DICT_PATH = os.path.expanduser('~/csrp/dictionaries/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text and pos tags from HKCanCor CHAT files\n",
    "### and save them in separate files\n",
    "#### Enable Code if this has not done prior"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def replaceCharacters(text):\n",
    "    reg_exp_dict = {\n",
    "        '':'㗎', \n",
    "#         '':'𠺢',\n",
    "        '':'噃', '':'𠺝', '':'喎',\n",
    "#         '':'尛'\n",
    "        '':'liu', '':'lung', '':'𧕴',\n",
    "        '':'喵', '\\?幾':'幾', '':'𥅾',\n",
    "#         '':'zong',\n",
    "        '':'嚕',\n",
    "#         '':'lu',\n",
    "        '':'呢', \n",
    "#         '':'le', \n",
    "        '':'𥄫',\n",
    "    }\n",
    "        \n",
    "    for replace_char, sub_char in reg_exp_dict.items():\n",
    "        text = re.sub(replace_char, sub_char, text, count=0, flags=re.DOTALL)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to do batch extracting text from HK Cantonese corpus data \n",
    "# i.e. the 58 annotated transcripts in http://compling.hss.ntu.edu.sg/hkcancor/\n",
    "# that are reproduced in CHAT format in PyCantonese data folder\n",
    "# Idea is to keep extracted data in separate files for sake of flexibility\n",
    "# This means something like corpus = pc.hkcancor() cannot be called\n",
    "def save_transcriptAndPOSTags():\n",
    "    chat_files = glob.glob(PYCANTONESE_PATH + 'pycantonese/data/hkcancor/*.cha')\n",
    "    \n",
    "    # a helper to save each item in hkcorpus_postag and \n",
    "    # hkcorpus_text in its own line\n",
    "    def saveFile(str_list, filename):\n",
    "        with open(CORPUS_PATH + filename % i, 'w', encoding='utf-8') as f:\n",
    "            str_list = map(lambda x: x+\"\\n\", str_list)\n",
    "            f.writelines(str_list)\n",
    "        assert(f.closed)\n",
    "    \n",
    "    def pickleDictionary(filename, prob_dict):\n",
    "        with open(OUTPUT_PATH + filename, 'wb') as f:\n",
    "            # jieba uses protocol 0 encoding for its pickle files\n",
    "            pickle.dump(prob_dict, f)\n",
    "        assert(f.closed)\n",
    "    \n",
    "    \n",
    "    for i, chat_file in enumerate(chat_files):\n",
    "        corpus = pc.read_chat(chat_file, encoding='utf-8')\n",
    "\n",
    "        # word_sents() and pos_sents() called in the following\n",
    "        # lines are customized methods in the CantoneseCHATReader \n",
    "        # class.  They are not in the official pyCantonese library\n",
    "\n",
    "        \n",
    "        # join strings in each list,\n",
    "        # replace Cantonese characters and remove leading whitespaces \n",
    "        hkcorpus_text = [replaceCharacters(\n",
    "            \" \".join(x)).strip(' ') for x in corpus.word_sents()]\n",
    "\n",
    "    #    # find files with wrongly decoded Cantonese characters\n",
    "    #     findword = re.search('|', hkcorpus_text, flags=0)\n",
    "    #     if findword:\n",
    "    #         print(chat_file)\n",
    "\n",
    "        # join strings in each list, replace symbols, \n",
    "        # remove leading whitespaces, and change pos to lower case\n",
    "        hkcorpus_postag = [\n",
    "            \" \".join(x).strip(' ').lower() for x in corpus.pos_sents()]\n",
    "        \n",
    "    \n",
    "        saveFile(hkcorpus_text, r'text/hk_cantonese_corpus_%d.txt')\n",
    "        saveFile(hkcorpus_postag, r'pos/hk_cantonese_corpus_pos_%d.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_transcriptAndPOSTags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 58\n",
    "dataframes = []\n",
    "for i in range(corpus_size):\n",
    "    os.chdir(CORPUS_PATH)\n",
    "    with open(f'text/hk_cantonese_corpus_{i}.txt',\n",
    "              'r', encoding='utf-8') as ftext:\n",
    "        \n",
    "        # put each string in list into a list of words,\n",
    "        # removing all empty entries in latter\n",
    "        text_list = [x.split(\" \") for x in ftext.read().splitlines()]\n",
    "        text_list = [list(filter(str.strip, x )) for x in text_list]\n",
    "    assert(ftext.closed)\n",
    "    \n",
    "    \n",
    "    with open(f'pos/hk_cantonese_corpus_pos_{i}.txt',\n",
    "        'r', encoding='utf-8') as fpos:\n",
    "        \n",
    "        # put each string in list into a list of pos tags\n",
    "        # removing all empty entries in latter\n",
    "        pos_list = [x.split(\" \") for x in fpos.read().splitlines()]\n",
    "        pos_list = [list(filter(str.strip, x)) for x in pos_list]\n",
    "    assert(fpos.closed)\n",
    "    \n",
    "    \n",
    "    table = pd.DataFrame({ 'file_num': i, 'text' : text_list, 'pos' : pos_list })\n",
    "    dataframes.append(table)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(entry):\n",
    "    removed = [re.sub(r'[\\,\"\\-\\.\\!…\\?？○#]', \"\", x, count=0) for x in entry]\n",
    "    return list(filter(None, removed)) # remove all empty strings in list\n",
    "\n",
    "def removeTrailingNumbersFromPOS(entry):\n",
    "    return [x.strip('0123456789') for x in entry]\n",
    "\n",
    "# removePunctuations(['v1', 'v', 'n', 'y', 'nr', 'nr', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this for all dataframes\n",
    "for index, d in enumerate(dataframes):\n",
    "    d['pos'] = d['pos'].apply(removePunctuations).apply(removeTrailingNumbersFromPOS)\n",
    "    d['text'] = d['text'].apply(removePunctuations)\n",
    "    \n",
    "    # a check to see if the pos lengths are the same as the text lengths\n",
    "    if d.pos.str.len().all() != d.text.str.len().all():\n",
    "        print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_num</th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[有冇, 養, 寵物, 𡃉, 王, 美美]</td>\n",
       "      <td>[v, v, n, y, nr, nr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[有]</td>\n",
       "      <td>[v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[養, 咗, 兩, 隻, 狗]</td>\n",
       "      <td>[v, u, m, q, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[真, 㗎]</td>\n",
       "      <td>[a, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[乜嘢, 樣, 𡃉]</td>\n",
       "      <td>[r, n, y]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_num                   text                   pos\n",
       "0         0  [有冇, 養, 寵物, 𡃉, 王, 美美]  [v, v, n, y, nr, nr]\n",
       "1         0                    [有]                   [v]\n",
       "2         0        [養, 咗, 兩, 隻, 狗]       [v, u, m, q, n]\n",
       "3         0                 [真, 㗎]                [a, y]\n",
       "4         0             [乜嘢, 樣, 𡃉]             [r, n, y]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinASCIIWords(rowtext, rowpos):\n",
    "    def string_is_ascii(string):\n",
    "        try:\n",
    "            string.encode(encoding='ascii')\n",
    "        except UnicodeEncodeError:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    for index, (word, pos) in enumerate(zip(rowtext, rowpos)):\n",
    "        next_index = index + 1       \n",
    "        \n",
    "        while next_index < len(rowtext) and \\\n",
    "            string_is_ascii(word) and \\\n",
    "            string_is_ascii(rowtext[next_index]):\n",
    "            \n",
    "            rowtext[index] += \"_\" + rowtext[next_index]\n",
    "            rowtext.pop(next_index)\n",
    "            rowpos.pop(next_index)\n",
    "            word = rowtext[index] # reset the word\n",
    "\n",
    "    return rowtext, rowpos\n",
    "\n",
    "\n",
    "# do this for all dataframes\n",
    "for d in dataframes:\n",
    "    result =  d.apply(lambda row: joinASCIIWords(\n",
    "        row['text'], row['pos']), axis=1).apply(pd.Series)\n",
    "    d['text'] = result[0]\n",
    "    d['pos'] = result[1]\n",
    "    \n",
    "del result\n",
    "\n",
    "# x = ['c', 'v', 'xn', 'xn', 'xn', 'q']\n",
    "# y = ['跟住', '買', 'fax', 'modem', 'modem', '個']\n",
    "# joinASCIIWords(y, x)\n",
    "\n",
    "# joinASCIIWords(['跟住', '買', '個', 'fax', 'modem', '而家', '幾百', '蚊', '啫', '嗎'])\n",
    "# joinASCIIWords(['跟住', '買', '個', 'fax', 'modem', 'hello'])\n",
    "# joinASCIIWords(['fax', 'modem', 'hello','跟住', 'hello','買', '個'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_num</th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[有冇, 養, 寵物, 𡃉, 王, 美美]</td>\n",
       "      <td>[v, v, n, y, nr, nr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[有]</td>\n",
       "      <td>[v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[養, 咗, 兩, 隻, 狗]</td>\n",
       "      <td>[v, u, m, q, n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[真, 㗎]</td>\n",
       "      <td>[a, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[乜嘢, 樣, 𡃉]</td>\n",
       "      <td>[r, n, y]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_num                   text                   pos\n",
       "0         0  [有冇, 養, 寵物, 𡃉, 王, 美美]  [v, v, n, y, nr, nr]\n",
       "1         0                    [有]                   [v]\n",
       "2         0        [養, 咗, 兩, 隻, 狗]       [v, u, m, q, n]\n",
       "3         0                 [真, 㗎]                [a, y]\n",
       "4         0             [乜嘢, 樣, 𡃉]             [r, n, y]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempt = ['One2Free', 'A1', '323', '121', 'N64', '東方188' ]\n",
    "def fixCantoneseJupingsInWordColumn(word_list, exempt_list):\n",
    "    for index, word in enumerate(word_list):\n",
    "        if word not in exempt_list and re.search(r'[0-9]', word, re.DOTALL):\n",
    "            word = re.sub(r\"[0-9]\", \"_\", word, re.DOTALL)\n",
    "            if word[-1] == \"_\":\n",
    "                word = word[:-1]\n",
    "        word_list[index] = word\n",
    "    return word_list\n",
    "\n",
    "# test\n",
    "#fixCantoneseJupings('Zip1', words_with_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataframes:\n",
    "    d['text'] = d['text'].apply(fixCantoneseJupingsInWordColumn, exempt_list=exempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tagged_sentences = df_corpus.apply(lambda row: list(zip(row['text'], row['pos']) ), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untagged_sentences = df_corpus['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('hkcancorpus_tokens.p', 'wb') as f:\n",
    "#     pickle.dump(untagged_sentences, f)\n",
    "# with open('hkcancorpus_pos_tokens.p', 'wb') as f:\n",
    "#     pickle.dump(pos_tagged_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create a stopword list\n",
    "### For details in the methodology behind, see \n",
    "#### `Zou et. al 2006, \"Automatic Construction of Chinese Stop Word Lists\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDPY_PATH = os.path.expanduser('~/csrp/corpus/')\n",
    "import sys\n",
    "sys.path.insert(0, STOPWORDPY_PATH)\n",
    "from stopwords import Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stopword list\n",
    "df_text = []\n",
    "\n",
    "for d in dataframes:\n",
    "    df_text.append(np.sum(d['text'].values) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[有冇, 養, 寵物, 𡃉, 王, 美美, 有, 養, 咗, 兩, 隻, 狗, 真, 㗎, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[除咗, 係, 唔係, 應該, 點, 講, 個, 感覺, 係, 係, 啊, 誒, 誒, 或者...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Medium_rare, 呀, medium_rare, 跟住, 呢, 就, medium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[喂, 你, 下年, 畢業, 嚹, 喎, 你, 諗, 住, 做, 乜嘢, 啊, 我, 諗, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[噉, 𠻺, 最近, 呢, 就, 喺, 新聞, 裏邊, 呢, 睇, 到, 呢, 就, 係, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  [有冇, 養, 寵物, 𡃉, 王, 美美, 有, 養, 咗, 兩, 隻, 狗, 真, 㗎, ...\n",
       "1  [除咗, 係, 唔係, 應該, 點, 講, 個, 感覺, 係, 係, 啊, 誒, 誒, 或者...\n",
       "2  [Medium_rare, 呀, medium_rare, 跟住, 呢, 就, medium...\n",
       "3  [喂, 你, 下年, 畢業, 嚹, 喎, 你, 諗, 住, 做, 乜嘢, 啊, 我, 諗, ...\n",
       "4  [噉, 𠻺, 最近, 呢, 就, 喺, 新聞, 裏邊, 呢, 睇, 到, 呢, 就, 係, ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.DataFrame({'transcript': df_text})\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stopwords = Stopword(df_text['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sat_val_rank</th>\n",
       "      <th>mean_prob_rank</th>\n",
       "      <th>var_prob_rank</th>\n",
       "      <th>entropy_rank</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>我</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>唔</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>好</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>誒</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>去</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>係</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>呢</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>即係</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>嘅</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>佢</td>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>哩個</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>就</td>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>啊</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>喀</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>問</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>喇</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>噉樣</td>\n",
       "      <td>100</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>囖</td>\n",
       "      <td>149</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>你</td>\n",
       "      <td>203</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>好玩</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>歐洲</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>159</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>買</td>\n",
       "      <td>57</td>\n",
       "      <td>75</td>\n",
       "      <td>41</td>\n",
       "      <td>82</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>乜嘢</td>\n",
       "      <td>143</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>喎</td>\n",
       "      <td>201</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>即</td>\n",
       "      <td>10</td>\n",
       "      <td>111</td>\n",
       "      <td>22</td>\n",
       "      <td>148</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>做</td>\n",
       "      <td>175</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>45</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>嗯</td>\n",
       "      <td>127</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "      <td>70</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>睇</td>\n",
       "      <td>210</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>其實</td>\n",
       "      <td>182</td>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>會</td>\n",
       "      <td>255</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>computer_knowledge</td>\n",
       "      <td>7014</td>\n",
       "      <td>6987</td>\n",
       "      <td>6982</td>\n",
       "      <td>6976</td>\n",
       "      <td>27959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>executive</td>\n",
       "      <td>6997</td>\n",
       "      <td>7015</td>\n",
       "      <td>6983</td>\n",
       "      <td>6971</td>\n",
       "      <td>27966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>arts</td>\n",
       "      <td>6978</td>\n",
       "      <td>6996</td>\n",
       "      <td>6987</td>\n",
       "      <td>7007</td>\n",
       "      <td>27968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>親自</td>\n",
       "      <td>7016</td>\n",
       "      <td>6959</td>\n",
       "      <td>7000</td>\n",
       "      <td>6993</td>\n",
       "      <td>27968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>dis</td>\n",
       "      <td>6988</td>\n",
       "      <td>7003</td>\n",
       "      <td>7021</td>\n",
       "      <td>6957</td>\n",
       "      <td>27969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>discipline</td>\n",
       "      <td>6991</td>\n",
       "      <td>7013</td>\n",
       "      <td>7009</td>\n",
       "      <td>6959</td>\n",
       "      <td>27972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>spend</td>\n",
       "      <td>7011</td>\n",
       "      <td>6968</td>\n",
       "      <td>7007</td>\n",
       "      <td>6986</td>\n",
       "      <td>27972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>突然之間</td>\n",
       "      <td>6964</td>\n",
       "      <td>6989</td>\n",
       "      <td>7001</td>\n",
       "      <td>7019</td>\n",
       "      <td>27973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Daniel</td>\n",
       "      <td>6981</td>\n",
       "      <td>7020</td>\n",
       "      <td>6995</td>\n",
       "      <td>6978</td>\n",
       "      <td>27974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>趕住</td>\n",
       "      <td>7006</td>\n",
       "      <td>6992</td>\n",
       "      <td>6961</td>\n",
       "      <td>7015</td>\n",
       "      <td>27974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>aim_at</td>\n",
       "      <td>6970</td>\n",
       "      <td>6991</td>\n",
       "      <td>7011</td>\n",
       "      <td>7002</td>\n",
       "      <td>27974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>agent</td>\n",
       "      <td>6968</td>\n",
       "      <td>6986</td>\n",
       "      <td>7012</td>\n",
       "      <td>7013</td>\n",
       "      <td>27979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>杯轕</td>\n",
       "      <td>6957</td>\n",
       "      <td>7006</td>\n",
       "      <td>6997</td>\n",
       "      <td>7020</td>\n",
       "      <td>27980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>人選</td>\n",
       "      <td>6985</td>\n",
       "      <td>7019</td>\n",
       "      <td>6976</td>\n",
       "      <td>7001</td>\n",
       "      <td>27981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>spelling_mistakes</td>\n",
       "      <td>7021</td>\n",
       "      <td>6965</td>\n",
       "      <td>7008</td>\n",
       "      <td>6988</td>\n",
       "      <td>27982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>請人</td>\n",
       "      <td>6995</td>\n",
       "      <td>7017</td>\n",
       "      <td>6980</td>\n",
       "      <td>6992</td>\n",
       "      <td>27984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>any_discipline</td>\n",
       "      <td>6972</td>\n",
       "      <td>6993</td>\n",
       "      <td>7010</td>\n",
       "      <td>7010</td>\n",
       "      <td>27985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>工作天</td>\n",
       "      <td>7013</td>\n",
       "      <td>7007</td>\n",
       "      <td>6962</td>\n",
       "      <td>7005</td>\n",
       "      <td>27987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>administrative_assistant</td>\n",
       "      <td>6965</td>\n",
       "      <td>7002</td>\n",
       "      <td>7006</td>\n",
       "      <td>7014</td>\n",
       "      <td>27987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>揀人</td>\n",
       "      <td>7009</td>\n",
       "      <td>6990</td>\n",
       "      <td>7003</td>\n",
       "      <td>6987</td>\n",
       "      <td>27989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>nature</td>\n",
       "      <td>7005</td>\n",
       "      <td>6969</td>\n",
       "      <td>7018</td>\n",
       "      <td>7000</td>\n",
       "      <td>27992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>推遲</td>\n",
       "      <td>7010</td>\n",
       "      <td>6994</td>\n",
       "      <td>7005</td>\n",
       "      <td>6985</td>\n",
       "      <td>27994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Form_five_form_seven</td>\n",
       "      <td>7017</td>\n",
       "      <td>7008</td>\n",
       "      <td>6988</td>\n",
       "      <td>6981</td>\n",
       "      <td>27994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>firsthon</td>\n",
       "      <td>6992</td>\n",
       "      <td>7011</td>\n",
       "      <td>6977</td>\n",
       "      <td>7021</td>\n",
       "      <td>28001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>留底</td>\n",
       "      <td>6999</td>\n",
       "      <td>7012</td>\n",
       "      <td>6994</td>\n",
       "      <td>6997</td>\n",
       "      <td>28002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>律師</td>\n",
       "      <td>6979</td>\n",
       "      <td>7014</td>\n",
       "      <td>6992</td>\n",
       "      <td>7018</td>\n",
       "      <td>28003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>回音</td>\n",
       "      <td>7003</td>\n",
       "      <td>7000</td>\n",
       "      <td>6999</td>\n",
       "      <td>7008</td>\n",
       "      <td>28010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>marketing_and_and</td>\n",
       "      <td>7008</td>\n",
       "      <td>6978</td>\n",
       "      <td>7015</td>\n",
       "      <td>7016</td>\n",
       "      <td>28017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>心得</td>\n",
       "      <td>6980</td>\n",
       "      <td>7021</td>\n",
       "      <td>7013</td>\n",
       "      <td>7012</td>\n",
       "      <td>28026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>十一點鐘</td>\n",
       "      <td>7012</td>\n",
       "      <td>7005</td>\n",
       "      <td>7016</td>\n",
       "      <td>6995</td>\n",
       "      <td>28028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7022 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         index  sat_val_rank  mean_prob_rank  var_prob_rank  \\\n",
       "3427                         我            24               3              1   \n",
       "2192                         唔            19               6              8   \n",
       "2681                         好            17               9              9   \n",
       "5908                         誒             9              20              3   \n",
       "1966                         去             7              22              2   \n",
       "1362                         係            56               0              0   \n",
       "2117                         呢            46               2             10   \n",
       "1941                        即係            28              12             12   \n",
       "2321                         嘅            44              10             13   \n",
       "1334                         佢            69               8             14   \n",
       "2178                        哩個             3              42              7   \n",
       "2967                         就            77              11             15   \n",
       "2245                         啊           102               1             11   \n",
       "2264                         喀            33              34             18   \n",
       "2247                         問             2              64              6   \n",
       "2266                         喇           135              14             19   \n",
       "2350                        噉樣           100              33             26   \n",
       "2373                         囖           149              16             20   \n",
       "1330                         你           203               5             16   \n",
       "2702                        好玩             1              85              5   \n",
       "4260                        歐洲             0              86              4   \n",
       "6018                         買            57              75             41   \n",
       "1081                        乜嘢           143              40             32   \n",
       "2271                         喎           201              24             27   \n",
       "1939                         即            10             111             22   \n",
       "1430                         做           175              45             37   \n",
       "2299                         嗯           127              66             47   \n",
       "4992                         睇           210              39             38   \n",
       "1641                        其實           182              52             44   \n",
       "4025                         會           255              27             31   \n",
       "...                        ...           ...             ...            ...   \n",
       "422         computer_knowledge          7014            6987           6982   \n",
       "476                  executive          6997            7015           6983   \n",
       "360                       arts          6978            6996           6987   \n",
       "5832                        親自          7016            6959           7000   \n",
       "445                        dis          6988            7003           7021   \n",
       "446                 discipline          6991            7013           7009   \n",
       "743                      spend          7011            6968           7007   \n",
       "5135                      突然之間          6964            6989           7001   \n",
       "93                      Daniel          6981            7020           6995   \n",
       "6097                        趕住          7006            6992           6961   \n",
       "352                     aim_at          6970            6991           7011   \n",
       "351                      agent          6968            6986           7012   \n",
       "4103                        杯轕          6957            7006           6997   \n",
       "1223                        人選          6985            7019           6976   \n",
       "742          spelling_mistakes          7021            6965           7008   \n",
       "5930                        請人          6995            7017           6980   \n",
       "354             any_discipline          6972            6993           7010   \n",
       "3019                       工作天          7013            7007           6962   \n",
       "349   administrative_assistant          6965            7002           7006   \n",
       "3636                        揀人          7009            6990           7003   \n",
       "614                     nature          7005            6969           7018   \n",
       "3631                        推遲          7010            6994           7005   \n",
       "123       Form_five_form_seven          7017            7008           6988   \n",
       "502                   firsthon          6992            7011           6977   \n",
       "4842                        留底          6999            7012           6994   \n",
       "3224                        律師          6979            7014           6992   \n",
       "2397                        回音          7003            7000           6999   \n",
       "596          marketing_and_and          7008            6978           7015   \n",
       "3265                        心得          6980            7021           7013   \n",
       "1863                      十一點鐘          7012            7005           7016   \n",
       "\n",
       "      entropy_rank  weight  \n",
       "3427             3      31  \n",
       "2192             7      40  \n",
       "2681             9      44  \n",
       "5908            21      53  \n",
       "1966            24      55  \n",
       "1362             0      56  \n",
       "2117             2      60  \n",
       "1941            13      65  \n",
       "2321            12      79  \n",
       "1334             8      99  \n",
       "2178            51     103  \n",
       "2967            10     113  \n",
       "2245             1     115  \n",
       "2264            38     123  \n",
       "2247            90     162  \n",
       "2266            14     182  \n",
       "2350            33     192  \n",
       "2373            17     202  \n",
       "1330             5     229  \n",
       "2702           145     236  \n",
       "4260           159     249  \n",
       "6018            82     255  \n",
       "1081            42     257  \n",
       "2271            26     278  \n",
       "1939           148     291  \n",
       "1430            45     302  \n",
       "2299            70     310  \n",
       "4992            41     328  \n",
       "1641            54     332  \n",
       "4025            28     341  \n",
       "...            ...     ...  \n",
       "422           6976   27959  \n",
       "476           6971   27966  \n",
       "360           7007   27968  \n",
       "5832          6993   27968  \n",
       "445           6957   27969  \n",
       "446           6959   27972  \n",
       "743           6986   27972  \n",
       "5135          7019   27973  \n",
       "93            6978   27974  \n",
       "6097          7015   27974  \n",
       "352           7002   27974  \n",
       "351           7013   27979  \n",
       "4103          7020   27980  \n",
       "1223          7001   27981  \n",
       "742           6988   27982  \n",
       "5930          6992   27984  \n",
       "354           7010   27985  \n",
       "3019          7005   27987  \n",
       "349           7014   27987  \n",
       "3636          6987   27989  \n",
       "614           7000   27992  \n",
       "3631          6985   27994  \n",
       "123           6981   27994  \n",
       "502           7021   28001  \n",
       "4842          6997   28002  \n",
       "3224          7018   28003  \n",
       "2397          7008   28010  \n",
       "596           7016   28017  \n",
       "3265          7012   28026  \n",
       "1863          6995   28028  \n",
       "\n",
       "[7022 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopwords.generateStopwordList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs top 100 stopwords\n",
    "df_stopwords.outputStopWordlist(100, 'stopwords.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile HMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "df = df[['file_num', 'text', 'pos']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if string is an ascii\n",
    "def string_is_ascii(string):\n",
    "    try:\n",
    "        string.encode(encoding='ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# preprocessing function for BMES tagging of words\n",
    "#\n",
    "# It separates words to a list of characters\n",
    "# To preserve the ascii word,\n",
    "# find the first pointer ascii character position\n",
    "# find the last pointer ascii character position\n",
    "# concatenate ascii characters in the sublist, \n",
    "# removing empty strings and white spaces in between\n",
    "# parameter: word - word\n",
    "# returns: a list of tokens\n",
    "def tokenize_word(word):\n",
    "    char_list = list(word)\n",
    "#     print(\"Word is separated to : \" + str(char_list))\n",
    "\n",
    "    first_ascii_pos = []\n",
    "    last_ascii_pos = []\n",
    "    \n",
    "    ascii_flag = False\n",
    "    for i, c in enumerate(char_list):\n",
    "#         print(\"The current character is %s\" %c)\n",
    "        if string_is_ascii(c):           \n",
    "            if ascii_flag == False:\n",
    "#                 print(\"ascii set to true\")\n",
    "                first_ascii_pos.append(i)\n",
    "                ascii_flag = True\n",
    "            if i == len(char_list) - 1:\n",
    "                last_ascii_pos.append(len(char_list))\n",
    "        else:\n",
    "            if ascii_flag == True:\n",
    "#                 print(\"ascii set to false\")\n",
    "                last_ascii_pos.append(i)\n",
    "                ascii_flag = False\n",
    "        \n",
    "    if len(first_ascii_pos): # if array is not empty\n",
    "#         print(first_ascii_pos[::-1])\n",
    "#         print(last_ascii_pos[::-1])\n",
    "        for i, j in zip(first_ascii_pos[::-1], last_ascii_pos[::-1]):\n",
    "#             print(i, j)\n",
    "            char_list[i:j] = list(\n",
    "                filter(None, \"\".join(char_list[i:j]).split(\" \") ) )\n",
    "    return char_list\n",
    "\n",
    "    \n",
    "\n",
    "# function to tag words using the\n",
    "# BMES (begin, middle, end, single) tagging system\n",
    "# precondition: string must not be empty\n",
    "# returns list of separated words and corresponding\n",
    "# BMES tags\n",
    "def tagWord_BMES(word):\n",
    "    word_length = len(word)\n",
    "    assert(word_length)\n",
    "    bmes_list = []\n",
    "    \n",
    "    word_list = tokenize_word(word)\n",
    "    if len(word_list) == 1:\n",
    "        bmes_list.append(\"S\")\n",
    "    else:\n",
    "        for i, w in enumerate(word_list):\n",
    "            if i == 0:\n",
    "                bmes_list.append(\"B\")\n",
    "            elif i == len(word_list) - 1:\n",
    "                bmes_list.append(\"E\")\n",
    "            else:\n",
    "                bmes_list.append(\"M\")\n",
    "        \n",
    "    return bmes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test cases\n",
    "# print(tokenize_word('你office land過牆梯'))\n",
    "# print(tokenize_word('office'))\n",
    "# print(tokenize_word('你office'))\n",
    "# print(tokenize_word('hello過牆梯world'))\n",
    "# print(tokenize_word('hello過牆梯world過牆'))\n",
    "# print(tokenize_word('你過牆梯'))\n",
    "# print(tokenize_word('Hello  World'))\n",
    "\n",
    "# print(tagWord_BMES('你有過牆梯')) # BMMME\n",
    "# print(tagWord_BMES('office')) # S\n",
    "# print(tagWord_BMES('你office')) # BE\n",
    "# print(tagWord_BMES('office牆')) # BE\n",
    "# print(tagWord_BMES('Hello World')) # BE\n",
    "# print(tagWord_BMES('有有')) # BE\n",
    "# print(tagWord_BMES('有')) # S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper to count total number of start instances\n",
    "def countTotalStartInstances(start_dict):\n",
    "    total = 0\n",
    "    for _, value in start_dict.items():\n",
    "        total += value\n",
    "    return total\n",
    "\n",
    "\n",
    "# create training algorithm to calculate \n",
    "# emission (BMES->word) and transition (BMES->BMES)\n",
    "# probabilities\n",
    "# returns: a tuple of prob_trans, prob_emit, and prob_start\n",
    "def trainingHMM_BMESTagging(text_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    \n",
    "    # for prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list in text_lists:\n",
    "        previous = '<s>'\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "        \n",
    "        for j, character in enumerate(line_list):\n",
    "#             print(\"The entry contains %s\" % character)\n",
    "            text_bmesTags_list = tagWord_BMES(character)\n",
    "            if j == 0:\n",
    "                start_tag = text_bmesTags_list[0]\n",
    "                if start_tag not in start:\n",
    "                    start[start_tag] = 0\n",
    "                start[start_tag] += 1\n",
    "        \n",
    "            for i, bmesTag in enumerate(text_bmesTags_list):\n",
    "                \n",
    "                transition_bigram = previous + \" \" + bmesTag\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if bmesTag not in context:\n",
    "                    context[bmesTag] = 0\n",
    "                context[bmesTag] += 1\n",
    "\n",
    "                bigram_emission = bmesTag + \" \" + character[i]\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "\n",
    "                previous = bmesTag\n",
    "            \n",
    "        bigram_transition = previous + \" </s>\"\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "\n",
    "    # output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for key, value in transition.items():\n",
    "        previous_tag, current_tag = key.split(\" \", maxsplit=1)\n",
    "        if previous_tag != '<s>' and current_tag != \"</s>\":\n",
    "            prob_trans[previous_tag][current_tag] = math.log2(float(value)/context[previous_tag])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    \n",
    "    for key, value in emission.items():\n",
    "        tag, word = key.split(\" \", maxsplit=1)\n",
    "#         print(\"Context contains %d instances\" % context[tag])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag])))\n",
    "        prob_emit[tag][word] = math.log2(float(value)/context[tag])\n",
    "    \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag, value in start.items():\n",
    "        prob_start[tag] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    prob_start[\"M\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    prob_start[\"E\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_trans1, prob_emit1, prob_start1 = trainingHMM_BMESTagging(df.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training algorithm to calculate \n",
    "# emission (pos->word) and transition (pos-pos)\n",
    "# probabilities\n",
    "def trainingHMM_POSTagging(text_lists, posTags_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    char_state = {}\n",
    "    \n",
    "    # for char_state_tab.*, prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    # existing tagset may contain a combination of tags\n",
    "    pos_tagset1 = [pos_tag for posTag_list in posTags_lists for pos_tag in posTag_list]\n",
    "    pos_tagset2 = ['ag', 'a', 'ad', 'an', 'bg', 'b', 'c', 'dg', 'd', \n",
    "        'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'mg', 'm', 'ng', 'n', \n",
    "        'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'qg', 'q', 'rg', 'r', \n",
    "        's', 'tg', 't', 'ug', 'u', 'vg', 'v', 'vd', 'vn', 'w', 'x', \n",
    "        'yg', 'y', 'z'] # official HKCanCor tagset\n",
    "    unique_pos_tagset = set(pos_tagset1 + pos_tagset2)\n",
    "    bmes_tagset = {'B', 'M', 'E', 'S'}\n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list, linePosTags_list in zip(text_lists, posTags_lists):\n",
    "        previous = ('<s>') # start sentence tag\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "       \n",
    "        for j, (character, pos_tag) in enumerate(\n",
    "            zip(line_list, linePosTags_list) ):\n",
    "#             print(\"The entry contains %s with tag %s \" % (character, pos_tag) )\n",
    "            character_bmesTags_list = tagWord_BMES(character)\n",
    "            character_token_list = tokenize_word(character)\n",
    "            \n",
    "            # build up start dictionary\n",
    "            if j == 0:\n",
    "                start_tag = character_bmesTags_list[0]\n",
    "                if (start_tag, pos_tag) not in start:\n",
    "                    start[(start_tag, pos_tag)] = 0\n",
    "                start[(start_tag, pos_tag)] += 1\n",
    "        \n",
    "            # build up transition, emission dictionaries\n",
    "            for i, (token, bmesTag) in enumerate(\n",
    "                zip(character_token_list, character_bmesTags_list)):\n",
    "                \n",
    "                tag_pair = (bmesTag, pos_tag)\n",
    "                \n",
    "                transition_bigram = (previous, tag_pair)\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if tag_pair not in context:\n",
    "                    context[tag_pair] = 0\n",
    "                context[tag_pair] += 1\n",
    "\n",
    "                bigram_emission = (tag_pair, token)\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "                \n",
    "                if token not in char_state:\n",
    "                    char_state[token] = []\n",
    "                char_state[token].append(tag_pair)\n",
    "                    \n",
    "                \n",
    "                previous = tag_pair\n",
    "            \n",
    "        bigram_transition = (previous, \"</s>\")\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "        \n",
    "    \n",
    "# output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for (previous_tag_pair, current_tag_pair), value in transition.items():\n",
    "        if previous_tag_pair != ('<s>') and current_tag_pair != (\"</s>\"):\n",
    "            prob_trans[previous_tag_pair][current_tag_pair] = math.log2(float(value)/context[previous_tag_pair])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_trans:\n",
    "                prob_trans[(bmes_tag, pos_tag)] = {}\n",
    "                \n",
    "    \n",
    "    for (token, tag_pair_list) in char_state.items():\n",
    "        char_state[token] = tuple(set(tag_pair_list)) # only keep unique tag sets\n",
    "        \n",
    "    \n",
    "    for (tag_pair, word), value in emission.items():\n",
    "#         print(\"Context contains %d instances\" % context[tag_pair])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag_pair])))\n",
    "        prob_emit[tag_pair][word] = math.log2(float(value)/context[tag_pair])\n",
    "    \n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_emit:\n",
    "                prob_emit[(bmes_tag, pos_tag)] = {}\n",
    "\n",
    "                \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag_pair, value in start.items():\n",
    "        prob_start[tag_pair] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_start:\n",
    "                prob_start[(bmes_tag, pos_tag)] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start, char_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_trans2, prob_emit2, prob_start2, char_state2 = trainingHMM_POSTagging(df.text.tolist(), df.pos.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4, s5, s6, s7 = trainingHMM_POSTagging([['重', '有得', '搞']], [['d', 'vu', 'v']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output prob_trans, prob_emit, prob_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputDictionary(filename, prob_dict):\n",
    "    os.chdir(OUTPUT_PATH)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(\"P=\", end='')\n",
    "            pprint(prob_dict) \n",
    "    assert(f.closed)\n",
    "\n",
    "def pickleDictionary(filename, prob_dict):\n",
    "    os.chdir(OUTPUT_PATH)\n",
    "    with open(filename, 'wb') as f:\n",
    "        # jieba uses protocol 0 encoding for its pickle files\n",
    "        pickle.dump(prob_dict, f, protocol=0)\n",
    "    assert(f.closed)\n",
    "    \n",
    "def depickleDictionary(filename):\n",
    "    os.chdir(OUTPUT_PATH)\n",
    "    with open(filename, 'rb') as f:\n",
    "        prob_dict = pickle.load(f, encoding='utf-8')\n",
    "    assert(f.closed)\n",
    "    return prob_dict\n",
    "    \n",
    "outputDictionary(\"posseg/prob_trans.py\", prob_trans2)\n",
    "outputDictionary(\"posseg/prob_emit.py\", prob_emit2)\n",
    "outputDictionary(\"posseg/prob_start.py\", prob_start2)\n",
    "outputDictionary(\"posseg/char_state_tab.py\", char_state2)\n",
    "\n",
    "pickleDictionary(\"posseg/prob_trans.p\", prob_trans2)\n",
    "pickleDictionary(\"posseg/prob_emit.p\", prob_emit2)\n",
    "pickleDictionary(\"posseg/prob_start.p\", prob_start2)\n",
    "pickleDictionary(\"posseg/char_state_tab.p\", char_state2)\n",
    "\n",
    "outputDictionary(\"finalseg/prob_trans.py\", prob_trans1)\n",
    "outputDictionary(\"finalseg/prob_emit.py\", prob_emit1)\n",
    "outputDictionary(\"finalseg/prob_start.py\", prob_start1)\n",
    "\n",
    "pickleDictionary(\"finalseg/prob_trans.p\", prob_trans1)\n",
    "pickleDictionary(\"finalseg/prob_emit.p\", prob_emit1)\n",
    "pickleDictionary(\"finalseg/prob_start.p\", prob_start1)\n",
    "\n",
    "\n",
    "# s1 = depickleDictionary(\"finalseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"finalseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"finalseg/prob_start.p\")\n",
    "\n",
    "# s1 = depickleDictionary(\"posseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"posseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"posseg/prob_start.p\")\n",
    "# s4 = depickleDictionary(\"posseg/char_state_tab.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Dictionary Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything in Pandas\n",
    "# This is not necessary, but \n",
    "# it shows the layouts neatly\n",
    "df_full = pd.concat(dataframes).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame(np.sum(df_full.apply(lambda row: list(zip(\n",
    "    row['text'], row['pos']) ), axis=1).values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns = ['word', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.groupby(['word','pos'], sort=False).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns according to Jieba Dictionary layout\n",
    "df_full = df_full[['word', 'count', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_with_digits = df_full[df_full['word'].str.contains(r\"[0-9]+\", regex=True)]['word'].values\n",
    "\n",
    "# # define what to exclude from the set words_with_digits\n",
    "# words_with_digits = set(words_with_digits).difference({'One2Free', 'A1', '323', '121', 'N64', '東方188' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_with_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full['word'] = df_full['word'].apply(fixCantoneseJupingsInWordColumn, fix_list=words_with_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full[df_full['word'].str.contains(r\"[0-9]+\", regex=True)]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full[df_full['word'].str.contains(r\"[A-Za-z]+\", regex=True)]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns = df_full[df_full.pos.str.contains('^n|[^va]n', regex=True)]\n",
    "df_nouns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_others = df_full[~df_full.isin(df_nouns)].dropna()\n",
    "df_others['count'] = df_others['count'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DICT_PATH)\n",
    "\n",
    "df_nouns.word.to_csv(\n",
    "    'nouns.txt', \n",
    "    sep=' ', index=False, header=False)\n",
    "\n",
    "df_others.word.to_csv(\n",
    "    'others.txt', \n",
    "    sep=' ', index=False, header=False)\n",
    "\n",
    "df_full.to_csv(\n",
    "    'hkcantonesedict.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
