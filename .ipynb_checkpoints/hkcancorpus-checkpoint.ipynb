{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYCANTONESE_PATH = r'/home/lun/csrp/corpuses/pycantonese/'\n",
    "CORPUS_PATH = r'/home/lun/csrp/code/corpus/hkcancor/'\n",
    "OUTPUT_PATH = r'/home/lun/csrp/code/jieba-cantonese/'\n",
    "\n",
    "import sys, re, glob, math, collections\n",
    "sys.path.insert(0, PYCANTONESE_PATH)\n",
    "import pycantonese as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text and pos tags from HKCanCor CHAT files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def replaceCantoneseCharacters(text):\n",
    "    reg_exp_dict = {\n",
    "        '':'㗎', \n",
    "#         '':'𠺢',\n",
    "        '':'噃', '':'𠺝', '':'喎',\n",
    "#         '':'尛'\n",
    "        '':'liu', '':'lung', '':'𧕴',\n",
    "        '':'喵', '\\?幾':'幾', '':'𥅾',\n",
    "#         '':'zong',\n",
    "        '':'嚕',\n",
    "#         '':'lu',\n",
    "        '':'呢', \n",
    "#         '':'le', \n",
    "        '':'𥄫',\n",
    "        \n",
    "        # also replace with empty character \n",
    "        # if sentence ends with/contains .!。…?？\n",
    "        ',|\\\"|-|\\.|\\!|。|…|\\?|？':''\n",
    "    }\n",
    "        \n",
    "    for replace_char, sub_char in reg_exp_dict.items():\n",
    "        text = re.sub(replace_char, sub_char, text, count=0, flags=re.DOTALL)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def removeSymbols(string):\n",
    "    return re.sub('-|,|\\.|\\?|\\\"|\\!', '', string, count=0, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to do batch extracting text from HK Cantonese corpus data \n",
    "# i.e. the 58 annotated transcripts in http://compling.hss.ntu.edu.sg/hkcancor/\n",
    "# that are reproduced in CHAT format in PyCantonese data folder\n",
    "# Idea is to keep extracted data in separate files for flexibility\n",
    "# This means something like corpus = pc.hkcancor() cannot be called\n",
    "def save_transcriptAndPOSTags():\n",
    "    chat_files = glob.glob(PYCANTONESE_PATH + 'pycantonese/data/hkcancor/*.cha')\n",
    "    \n",
    "    # a helper to save each item in hkcorpus_postag and \n",
    "    # hkcorpus_text in its own line\n",
    "    def saveFile(str_list, filename):\n",
    "        with open(CORPUS_PATH + filename % i, 'w', encoding='utf-8') as f:\n",
    "            str_list = map(lambda x: x+\"\\n\", str_list)\n",
    "            f.writelines(str_list)\n",
    "        assert(f.closed)\n",
    "    \n",
    "    \n",
    "    for i, chat_file in enumerate(chat_files):\n",
    "        corpus = pc.read_chat(chat_file, encoding='utf-8')\n",
    "\n",
    "        # word_sents() and pos_sents() called in the following\n",
    "        # lines are customized methods in the CantoneseCHATReader \n",
    "        # class.  They are not in the official pyCantonese library\n",
    "\n",
    "        \n",
    "        # join strings in each list,\n",
    "        # replace Cantonese characters and remove leading whitespaces \n",
    "        hkcorpus_text = [replaceCantoneseCharacters(\n",
    "            \" \".join(x)).strip(' ') for x in corpus.word_sents()]\n",
    "\n",
    "    #    # find files with wrongly decoded Cantonese characters\n",
    "    #     findword = re.search('|', hkcorpus_text, flags=0)\n",
    "    #     if findword:\n",
    "    #         print(chat_file)\n",
    "\n",
    "        # join strings in each list, replace symbols, \n",
    "        # remove leading whitespaces, and change pos to lower case\n",
    "        hkcorpus_postag = [removeSymbols(\n",
    "            \" \".join(x)).strip(' ').lower() for x in corpus.pos_sents()]\n",
    "        \n",
    "    \n",
    "        saveFile(hkcorpus_text, r'text/hk_cantonese_corpus_%d.txt')\n",
    "        saveFile(hkcorpus_postag, r'pos/hk_cantonese_corpus_pos_%d.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_transcriptAndPOSTags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 58\n",
    "dataframes = []\n",
    "for i in range(corpus_size):\n",
    "    with open(CORPUS_PATH + r'text/hk_cantonese_corpus_%d.txt' % i, \n",
    "        'r', encoding='utf-8') as ftext:\n",
    "        \n",
    "        # put each string in list into a list of words,\n",
    "        # removing all empty entries in latter\n",
    "        text_list = [x.split(\" \") for x in ftext.read().splitlines()]\n",
    "        text_list = [list(filter(str.strip, x )) for x in text_list]\n",
    "    assert(ftext.closed)\n",
    "    \n",
    "    \n",
    "    with open(CORPUS_PATH + r'pos/hk_cantonese_corpus_pos_%d.txt' % i,\n",
    "        'r', encoding='utf-8') as fpos:\n",
    "        \n",
    "        # put each string in list into a list of pos tags\n",
    "        # removing all empty entries in latter\n",
    "        pos_list = [x.split(\" \") for x in fpos.read().splitlines()]\n",
    "        pos_list = [list(filter(str.strip, x)) for x in pos_list]\n",
    "    assert(fpos.closed)\n",
    "    \n",
    "    \n",
    "    table = pd.DataFrame({ 'file_num': i, 'text' : text_list, 'pos' : pos_list })\n",
    "    dataframes.append(table)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a stopword list using a statistical model\n",
    "### For details in the methodology behind, see \n",
    "#### `Zou et. al 2006, \"Automatic Construction of Chinese Stop Word Lists\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stopword list\n",
    "df_text = []\n",
    "\n",
    "for d in dataframes:\n",
    "    df_text.append(np.sum(d['text'].values) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame({'transcript': df_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_text.head()\n",
    "total_num_text = df_text.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = []\n",
    "for index, dt in enumerate(df_text['transcript'].values ):\n",
    "    df_words.append(pd.DataFrame(dt) )\n",
    "    numwords = df_words[index].count()[0]\n",
    "    \n",
    "    df_words[index].columns = ['word']\n",
    "    df_words[index] = df_words[index].groupby('word')['word'].count()\n",
    "    df_words[index] = pd.DataFrame(df_words[index])\n",
    "    df_words[index].columns = ['num_instances']\n",
    "    df_words[index]['word_prob'] = df_words[index]['num_instances'] / numwords\n",
    "    df_words[index]['text_num'] = index\n",
    "    df_words[index].reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.concat(df_words, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>num_instances</th>\n",
       "      <th>word_prob</th>\n",
       "      <th>text_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hifi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wilson</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fastforward</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>office</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  num_instances  word_prob  text_num\n",
       "0         Good              1   0.000475         0\n",
       "1         Hifi              1   0.000475         0\n",
       "2       Wilson              1   0.000475         0\n",
       "3  fastforward              1   0.000475         0\n",
       "4       office              1   0.000475         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sumN_prob = df_words.groupby('word')['word_prob'].sum()\n",
    "df_sumN_prob.rename('sum_n_prob', inplace=True)\n",
    "\n",
    "df_mean_prob = df_sumN_prob / total_num_text \n",
    "df_mean_prob.rename('mean_prob', inplace=True)\n",
    "\n",
    "# join dataframes\n",
    "df_sum_N_var_prob = df_words.join(pd.DataFrame(df_mean_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_N_var_prob.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>num_instances</th>\n",
       "      <th>word_prob</th>\n",
       "      <th>text_num</th>\n",
       "      <th>mean_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>323</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>36</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AGM</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AIO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ALO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ALevel</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AM</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AM</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AV</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Access</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Accident</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Account</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Account</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Acting</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ada</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ada</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ada</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Admin</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Admin</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Advanced</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Airways</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Alevel</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Alevel</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25389</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>21</td>\n",
       "      <td>0.017842</td>\n",
       "      <td>37</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25390</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>38</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25391</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>17</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>39</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25392</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>37</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>40</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25393</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>36</td>\n",
       "      <td>0.019824</td>\n",
       "      <td>41</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25394</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>39</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>42</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25395</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>34</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>43</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25396</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>19</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>44</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25397</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>13</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>45</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25398</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>76</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>46</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25399</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>20</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>47</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25400</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>35</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>48</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25401</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>37</td>\n",
       "      <td>0.019053</td>\n",
       "      <td>49</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25402</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>30</td>\n",
       "      <td>0.023715</td>\n",
       "      <td>50</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25403</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>36</td>\n",
       "      <td>0.021765</td>\n",
       "      <td>51</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25404</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>37</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>52</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25405</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>18</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>53</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25406</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>8</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>54</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25407</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>16</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>55</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25408</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>50</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>56</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25409</th>\n",
       "      <td>𡃉</td>\n",
       "      <td>9</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>57</td>\n",
       "      <td>0.014546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25410</th>\n",
       "      <td>𢱕</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25411</th>\n",
       "      <td>𤓓味</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25412</th>\n",
       "      <td>𥄫</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25413</th>\n",
       "      <td>𥅾</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25414</th>\n",
       "      <td>𥇣</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25415</th>\n",
       "      <td>𦧲</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25416</th>\n",
       "      <td>𦧲</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25417</th>\n",
       "      <td>𨃩低</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25418</th>\n",
       "      <td>𨋢</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25419 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  num_instances  word_prob  text_num  mean_prob\n",
       "0             '              1   0.000354        43   0.000006\n",
       "1           121              2   0.000571         2   0.000010\n",
       "2           323              4   0.001143         2   0.000020\n",
       "3             A             36   0.009549        12   0.000255\n",
       "4             A              2   0.000883        26   0.000255\n",
       "5             A              2   0.001031        35   0.000255\n",
       "6             A              5   0.002555        39   0.000255\n",
       "7             A              3   0.000750        46   0.000255\n",
       "8            A1              4   0.001061        12   0.000018\n",
       "9           AGM              3   0.000609        31   0.000011\n",
       "10          AIO              4   0.002701        57   0.000047\n",
       "11          ALO              2   0.000622        23   0.000011\n",
       "12       ALevel              2   0.000531        12   0.000009\n",
       "13           AM              2   0.000547        19   0.000091\n",
       "14           AM              5   0.004757        28   0.000091\n",
       "15           AV              1   0.000759        16   0.000013\n",
       "16       Access              2   0.000736        24   0.000013\n",
       "17     Accident              1   0.000203        31   0.000004\n",
       "18      Account              1   0.000513         5   0.000045\n",
       "19      Account              8   0.002122        12   0.000045\n",
       "20       Acting              1   0.000675        57   0.000012\n",
       "21          Ada              3   0.001546        35   0.000086\n",
       "22          Ada              2   0.001881        47   0.000086\n",
       "23          Ada              2   0.001581        50   0.000086\n",
       "24        Admin              2   0.000406        31   0.000013\n",
       "25        Admin              2   0.000366        56   0.000013\n",
       "26     Advanced              1   0.000873        38   0.000015\n",
       "27      Airways              1   0.000675        57   0.000012\n",
       "28       Alevel              2   0.000489        18   0.000022\n",
       "29       Alevel              2   0.000547        19   0.000022\n",
       "...         ...            ...        ...       ...        ...\n",
       "25389         𡃉             21   0.017842        37   0.014546\n",
       "25390         𡃉              6   0.005240        38   0.014546\n",
       "25391         𡃉             17   0.008687        39   0.014546\n",
       "25392         𡃉             37   0.021424        40   0.014546\n",
       "25393         𡃉             36   0.019824        41   0.014546\n",
       "25394         𡃉             39   0.014498        42   0.014546\n",
       "25395         𡃉             34   0.012040        43   0.014546\n",
       "25396         𡃉             19   0.011157        44   0.014546\n",
       "25397         𡃉             13   0.013670        45   0.014546\n",
       "25398         𡃉             76   0.019000        46   0.014546\n",
       "25399         𡃉             20   0.018815        47   0.014546\n",
       "25400         𡃉             35   0.017344        48   0.014546\n",
       "25401         𡃉             37   0.019053        49   0.014546\n",
       "25402         𡃉             30   0.023715        50   0.014546\n",
       "25403         𡃉             36   0.021765        51   0.014546\n",
       "25404         𡃉             37   0.023024        52   0.014546\n",
       "25405         𡃉             18   0.006265        53   0.014546\n",
       "25406         𡃉              8   0.009185        54   0.014546\n",
       "25407         𡃉             16   0.007452        55   0.014546\n",
       "25408         𡃉             50   0.009151        56   0.014546\n",
       "25409         𡃉              9   0.006077        57   0.014546\n",
       "25410         𢱕              1   0.000372        42   0.000006\n",
       "25411        𤓓味              1   0.000286         2   0.000005\n",
       "25412         𥄫              1   0.000442        26   0.000008\n",
       "25413         𥅾              2   0.000571         2   0.000010\n",
       "25414         𥇣              1   0.000245        18   0.000004\n",
       "25415         𦧲              1   0.000368        24   0.000015\n",
       "25416         𦧲              2   0.000500        46   0.000015\n",
       "25417        𨃩低              1   0.000488         3   0.000008\n",
       "25418         𨋢              1   0.000475         0   0.000008\n",
       "\n",
       "[25419 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_N_var_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_N_var_prob['sum_N_var_prob'] = np.power(\n",
    "    df_sum_N_var_prob['word_prob'].values - df_sum_N_var_prob['mean_prob'].values, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>num_instances</th>\n",
       "      <th>word_prob</th>\n",
       "      <th>text_num</th>\n",
       "      <th>mean_prob</th>\n",
       "      <th>sum_N_var_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>係</td>\n",
       "      <td>334</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>56</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9481</th>\n",
       "      <td>噉</td>\n",
       "      <td>231</td>\n",
       "      <td>0.042277</td>\n",
       "      <td>56</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>佢</td>\n",
       "      <td>202</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>56</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>係</td>\n",
       "      <td>190</td>\n",
       "      <td>0.050640</td>\n",
       "      <td>15</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>啊</td>\n",
       "      <td>180</td>\n",
       "      <td>0.044042</td>\n",
       "      <td>18</td>\n",
       "      <td>0.031676</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  num_instances  word_prob  text_num  mean_prob  sum_N_var_prob\n",
       "3727    係            334   0.061127        56   0.040246        0.000436\n",
       "9481    噉            231   0.042277        56   0.020902        0.000457\n",
       "3506    佢            202   0.036969        56   0.016589        0.000415\n",
       "3686    係            190   0.050640        15   0.040246        0.000108\n",
       "8013    啊            180   0.044042        18   0.031676        0.000153"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_N_var_prob.sort_values('num_instances', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_N_var_prob = df_sum_N_var_prob.groupby('word')['sum_N_var_prob'].sum()\n",
    "df_var_prob = df_sum_N_var_prob / numwords\n",
    "df_var_prob.rename('var_prob', inplace=True)\n",
    "\n",
    "df_stopwords = pd.DataFrame({\n",
    "    'mean_prob' : df_mean_prob,\n",
    "    'var_prob' : df_var_prob,  \n",
    "    'sat_val' : df_sumN_prob / df_sum_N_var_prob\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stopwords = df_stopwords[['mean_prob', 'var_prob', 'sat_val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_prob</th>\n",
       "      <th>var_prob</th>\n",
       "      <th>sat_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>係</th>\n",
       "      <td>0.040246</td>\n",
       "      <td>9.182646e-06</td>\n",
       "      <td>171.642311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>啊</th>\n",
       "      <td>0.031676</td>\n",
       "      <td>5.379635e-06</td>\n",
       "      <td>230.596737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>呢</th>\n",
       "      <td>0.023257</td>\n",
       "      <td>6.018811e-06</td>\n",
       "      <td>151.326950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>我</th>\n",
       "      <td>0.022570</td>\n",
       "      <td>9.152442e-06</td>\n",
       "      <td>96.575643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>噉</th>\n",
       "      <td>0.020902</td>\n",
       "      <td>2.005573e-06</td>\n",
       "      <td>408.153301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>你</th>\n",
       "      <td>0.019405</td>\n",
       "      <td>2.419016e-06</td>\n",
       "      <td>314.165411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>唔</th>\n",
       "      <td>0.017552</td>\n",
       "      <td>7.505671e-06</td>\n",
       "      <td>91.583393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>都</th>\n",
       "      <td>0.016892</td>\n",
       "      <td>1.025094e-06</td>\n",
       "      <td>645.340465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>佢</th>\n",
       "      <td>0.016589</td>\n",
       "      <td>3.285804e-06</td>\n",
       "      <td>197.714609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>好</th>\n",
       "      <td>0.016198</td>\n",
       "      <td>7.290269e-06</td>\n",
       "      <td>87.012368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>嘅</th>\n",
       "      <td>0.015173</td>\n",
       "      <td>3.927512e-06</td>\n",
       "      <td>151.298286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>就</th>\n",
       "      <td>0.015026</td>\n",
       "      <td>2.848252e-06</td>\n",
       "      <td>206.609380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>即係</th>\n",
       "      <td>0.014569</td>\n",
       "      <td>5.228434e-06</td>\n",
       "      <td>109.126105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𡃉</th>\n",
       "      <td>0.014546</td>\n",
       "      <td>1.216359e-06</td>\n",
       "      <td>468.325617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>喇</th>\n",
       "      <td>0.011954</td>\n",
       "      <td>1.792130e-06</td>\n",
       "      <td>261.236162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>個</th>\n",
       "      <td>0.011616</td>\n",
       "      <td>6.182517e-07</td>\n",
       "      <td>735.830544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>囖</th>\n",
       "      <td>0.010860</td>\n",
       "      <td>1.547679e-06</td>\n",
       "      <td>274.795737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>6.631780e-07</td>\n",
       "      <td>634.364328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>啲</th>\n",
       "      <td>0.010627</td>\n",
       "      <td>1.067155e-06</td>\n",
       "      <td>390.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>一</th>\n",
       "      <td>0.009017</td>\n",
       "      <td>7.837863e-07</td>\n",
       "      <td>450.525207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>誒</th>\n",
       "      <td>0.008677</td>\n",
       "      <td>8.420489e-06</td>\n",
       "      <td>40.357965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>唔係</th>\n",
       "      <td>0.008248</td>\n",
       "      <td>6.415448e-07</td>\n",
       "      <td>503.512828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>去</th>\n",
       "      <td>0.008070</td>\n",
       "      <td>8.693631e-06</td>\n",
       "      <td>36.355415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>又</th>\n",
       "      <td>0.007041</td>\n",
       "      <td>5.344590e-07</td>\n",
       "      <td>515.939635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>喎</th>\n",
       "      <td>0.007009</td>\n",
       "      <td>8.837292e-07</td>\n",
       "      <td>310.597144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>話</th>\n",
       "      <td>0.007002</td>\n",
       "      <td>5.265057e-07</td>\n",
       "      <td>520.840354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>到</th>\n",
       "      <td>0.006821</td>\n",
       "      <td>3.049769e-07</td>\n",
       "      <td>875.954960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>會</th>\n",
       "      <td>0.006656</td>\n",
       "      <td>7.434349e-07</td>\n",
       "      <td>350.634242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>咗</th>\n",
       "      <td>0.006538</td>\n",
       "      <td>2.400715e-07</td>\n",
       "      <td>1066.533668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>嗰啲</th>\n",
       "      <td>0.006493</td>\n",
       "      <td>6.213656e-07</td>\n",
       "      <td>409.208089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>回音</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>六十萬</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spelling</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人選</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spend</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>揀人</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>推遲</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>佣金</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>開朗</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文靜</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tight</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>親自</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistakes</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>突然之間</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>新穎</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>先輪</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>administrative</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>全年</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>心得</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>律師</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aim</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>趕住</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>杯轕</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7070 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean_prob      var_prob      sat_val\n",
       "word                                                \n",
       "係                0.040246  9.182646e-06   171.642311\n",
       "啊                0.031676  5.379635e-06   230.596737\n",
       "呢                0.023257  6.018811e-06   151.326950\n",
       "我                0.022570  9.152442e-06    96.575643\n",
       "噉                0.020902  2.005573e-06   408.153301\n",
       "你                0.019405  2.419016e-06   314.165411\n",
       "唔                0.017552  7.505671e-06    91.583393\n",
       "都                0.016892  1.025094e-06   645.340465\n",
       "佢                0.016589  3.285804e-06   197.714609\n",
       "好                0.016198  7.290269e-06    87.012368\n",
       "嘅                0.015173  3.927512e-06   151.298286\n",
       "就                0.015026  2.848252e-06   206.609380\n",
       "即係               0.014569  5.228434e-06   109.126105\n",
       "𡃉                0.014546  1.216359e-06   468.325617\n",
       "喇                0.011954  1.792130e-06   261.236162\n",
       "個                0.011616  6.182517e-07   735.830544\n",
       "囖                0.010860  1.547679e-06   274.795737\n",
       "有                0.010742  6.631780e-07   634.364328\n",
       "啲                0.010627  1.067155e-06   390.001220\n",
       "一                0.009017  7.837863e-07   450.525207\n",
       "誒                0.008677  8.420489e-06    40.357965\n",
       "唔係               0.008248  6.415448e-07   503.512828\n",
       "去                0.008070  8.693631e-06    36.355415\n",
       "又                0.007041  5.344590e-07   515.939635\n",
       "喎                0.007009  8.837292e-07   310.597144\n",
       "話                0.007002  5.265057e-07   520.840354\n",
       "到                0.006821  3.049769e-07   875.954960\n",
       "會                0.006656  7.434349e-07   350.634242\n",
       "咗                0.006538  2.400715e-07  1066.533668\n",
       "嗰啲               0.006493  6.213656e-07   409.208089\n",
       "...                   ...           ...          ...\n",
       "回音               0.000003  2.184325e-11  5657.401046\n",
       "六十萬              0.000003  2.184325e-11  5657.401046\n",
       "spelling         0.000003  2.184325e-11  5657.401046\n",
       "人選               0.000003  2.184325e-11  5657.401046\n",
       "spend            0.000003  2.184325e-11  5657.401046\n",
       "standard         0.000003  2.184325e-11  5657.401046\n",
       "揀人               0.000003  2.184325e-11  5657.401046\n",
       "推遲               0.000003  2.184325e-11  5657.401046\n",
       "study            0.000003  2.184325e-11  5657.401046\n",
       "佣金               0.000003  2.184325e-11  5657.401046\n",
       "開朗               0.000003  2.184325e-11  5657.401046\n",
       "文靜               0.000003  2.184325e-11  5657.401046\n",
       "nature           0.000003  2.184325e-11  5657.401046\n",
       "customer         0.000003  2.184325e-11  5657.401046\n",
       "tight            0.000003  2.184325e-11  5657.401046\n",
       "親自               0.000003  2.184325e-11  5657.401046\n",
       "mistakes         0.000003  2.184325e-11  5657.401046\n",
       "mark             0.000003  2.184325e-11  5657.401046\n",
       "突然之間             0.000003  2.184325e-11  5657.401046\n",
       "新穎               0.000003  2.184325e-11  5657.401046\n",
       "先輪               0.000003  2.184325e-11  5657.401046\n",
       "administrative   0.000003  2.184325e-11  5657.401046\n",
       "全年               0.000003  2.184325e-11  5657.401046\n",
       "心得               0.000003  2.184325e-11  5657.401046\n",
       "agent            0.000003  2.184325e-11  5657.401046\n",
       "律師               0.000003  2.184325e-11  5657.401046\n",
       "aim              0.000003  2.184325e-11  5657.401046\n",
       "趕住               0.000003  2.184325e-11  5657.401046\n",
       "any              0.000003  2.184325e-11  5657.401046\n",
       "杯轕               0.000003  2.184325e-11  5657.401046\n",
       "\n",
       "[7070 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopwords.sort_values('mean_prob', ascending=False) # high mp\n",
    "# df_stopwords.sort_values('var_prob', ascending=False) # high var_prob\n",
    "# df_stopwords.sort_values('sat_val', ascending=False) # high var_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a stopword list using an information model\n",
    "### For details in the methodology behind, see \n",
    "#### `Zou et. al 2006, \"Automatic Construction of Chinese Stop Word Lists\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_instances</th>\n",
       "      <th>word_prob</th>\n",
       "      <th>text_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hifi</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilson</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastforward</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>office</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num_instances  word_prob  text_num\n",
       "word                                           \n",
       "Good                     1   0.000475         0\n",
       "Hifi                     1   0.000475         0\n",
       "Wilson                   1   0.000475         0\n",
       "fastforward              1   0.000475         0\n",
       "office                   1   0.000475         0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the entropy for each word\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy = df_words['word_prob'] * np.log2(1 / df_words['word_prob'])\n",
    "df_entropy.rename('entropy', inplace=True)\n",
    "df_entropy = pd.DataFrame(df_entropy)\n",
    "df_entropy.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stopwords['entropy'] = df_entropy.groupby('word')['entropy'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_prob</th>\n",
       "      <th>var_prob</th>\n",
       "      <th>sat_val</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>係</th>\n",
       "      <td>0.040246</td>\n",
       "      <td>9.182646e-06</td>\n",
       "      <td>171.642311</td>\n",
       "      <td>10.531480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>啊</th>\n",
       "      <td>0.031676</td>\n",
       "      <td>5.379635e-06</td>\n",
       "      <td>230.596737</td>\n",
       "      <td>8.925534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>呢</th>\n",
       "      <td>0.023257</td>\n",
       "      <td>6.018811e-06</td>\n",
       "      <td>151.326950</td>\n",
       "      <td>7.027129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>我</th>\n",
       "      <td>0.022570</td>\n",
       "      <td>9.152442e-06</td>\n",
       "      <td>96.575643</td>\n",
       "      <td>6.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>噉</th>\n",
       "      <td>0.020902</td>\n",
       "      <td>2.005573e-06</td>\n",
       "      <td>408.153301</td>\n",
       "      <td>6.633132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>你</th>\n",
       "      <td>0.019405</td>\n",
       "      <td>2.419016e-06</td>\n",
       "      <td>314.165411</td>\n",
       "      <td>6.232663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>都</th>\n",
       "      <td>0.016892</td>\n",
       "      <td>1.025094e-06</td>\n",
       "      <td>645.340465</td>\n",
       "      <td>5.678637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>唔</th>\n",
       "      <td>0.017552</td>\n",
       "      <td>7.505671e-06</td>\n",
       "      <td>91.583393</td>\n",
       "      <td>5.671489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>佢</th>\n",
       "      <td>0.016589</td>\n",
       "      <td>3.285804e-06</td>\n",
       "      <td>197.714609</td>\n",
       "      <td>5.459486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>好</th>\n",
       "      <td>0.016198</td>\n",
       "      <td>7.290269e-06</td>\n",
       "      <td>87.012368</td>\n",
       "      <td>5.324538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>就</th>\n",
       "      <td>0.015026</td>\n",
       "      <td>2.848252e-06</td>\n",
       "      <td>206.609380</td>\n",
       "      <td>5.053237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>𡃉</th>\n",
       "      <td>0.014546</td>\n",
       "      <td>1.216359e-06</td>\n",
       "      <td>468.325617</td>\n",
       "      <td>5.036471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>嘅</th>\n",
       "      <td>0.015173</td>\n",
       "      <td>3.927512e-06</td>\n",
       "      <td>151.298286</td>\n",
       "      <td>5.015457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>即係</th>\n",
       "      <td>0.014569</td>\n",
       "      <td>5.228434e-06</td>\n",
       "      <td>109.126105</td>\n",
       "      <td>4.761931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>喇</th>\n",
       "      <td>0.011954</td>\n",
       "      <td>1.792130e-06</td>\n",
       "      <td>261.236162</td>\n",
       "      <td>4.263261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>個</th>\n",
       "      <td>0.011616</td>\n",
       "      <td>6.182517e-07</td>\n",
       "      <td>735.830544</td>\n",
       "      <td>4.260456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>6.631780e-07</td>\n",
       "      <td>634.364328</td>\n",
       "      <td>3.996522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>啲</th>\n",
       "      <td>0.010627</td>\n",
       "      <td>1.067155e-06</td>\n",
       "      <td>390.001220</td>\n",
       "      <td>3.922487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>囖</th>\n",
       "      <td>0.010860</td>\n",
       "      <td>1.547679e-06</td>\n",
       "      <td>274.795737</td>\n",
       "      <td>3.921209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>一</th>\n",
       "      <td>0.009017</td>\n",
       "      <td>7.837863e-07</td>\n",
       "      <td>450.525207</td>\n",
       "      <td>3.443651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>唔係</th>\n",
       "      <td>0.008248</td>\n",
       "      <td>6.415448e-07</td>\n",
       "      <td>503.512828</td>\n",
       "      <td>3.211352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>誒</th>\n",
       "      <td>0.008677</td>\n",
       "      <td>8.420489e-06</td>\n",
       "      <td>40.357965</td>\n",
       "      <td>2.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>又</th>\n",
       "      <td>0.007041</td>\n",
       "      <td>5.344590e-07</td>\n",
       "      <td>515.939635</td>\n",
       "      <td>2.837268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>話</th>\n",
       "      <td>0.007002</td>\n",
       "      <td>5.265057e-07</td>\n",
       "      <td>520.840354</td>\n",
       "      <td>2.816528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>去</th>\n",
       "      <td>0.008070</td>\n",
       "      <td>8.693631e-06</td>\n",
       "      <td>36.355415</td>\n",
       "      <td>2.794224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>到</th>\n",
       "      <td>0.006821</td>\n",
       "      <td>3.049769e-07</td>\n",
       "      <td>875.954960</td>\n",
       "      <td>2.782360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>喎</th>\n",
       "      <td>0.007009</td>\n",
       "      <td>8.837292e-07</td>\n",
       "      <td>310.597144</td>\n",
       "      <td>2.766509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>咗</th>\n",
       "      <td>0.006538</td>\n",
       "      <td>2.400715e-07</td>\n",
       "      <td>1066.533668</td>\n",
       "      <td>2.701735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>會</th>\n",
       "      <td>0.006656</td>\n",
       "      <td>7.434349e-07</td>\n",
       "      <td>350.634242</td>\n",
       "      <td>2.671996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>嗰啲</th>\n",
       "      <td>0.006493</td>\n",
       "      <td>6.213656e-07</td>\n",
       "      <td>409.208089</td>\n",
       "      <td>2.626787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>executive</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>佣金</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impression</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interviewer</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>開朗</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>律師</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foul</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knowledge</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>心得</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firsthon</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>學術界</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>十一點鐘</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人選</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistakes</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boyscott</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>回音</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>工作天</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>濫</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>層面</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>違約</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>居然</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>身在福中不知福</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>趕住</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>突然之間</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>administrative</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.184325e-11</td>\n",
       "      <td>5657.401046</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7070 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean_prob      var_prob      sat_val    entropy\n",
       "word                                                           \n",
       "係                0.040246  9.182646e-06   171.642311  10.531480\n",
       "啊                0.031676  5.379635e-06   230.596737   8.925534\n",
       "呢                0.023257  6.018811e-06   151.326950   7.027129\n",
       "我                0.022570  9.152442e-06    96.575643   6.839800\n",
       "噉                0.020902  2.005573e-06   408.153301   6.633132\n",
       "你                0.019405  2.419016e-06   314.165411   6.232663\n",
       "都                0.016892  1.025094e-06   645.340465   5.678637\n",
       "唔                0.017552  7.505671e-06    91.583393   5.671489\n",
       "佢                0.016589  3.285804e-06   197.714609   5.459486\n",
       "好                0.016198  7.290269e-06    87.012368   5.324538\n",
       "就                0.015026  2.848252e-06   206.609380   5.053237\n",
       "𡃉                0.014546  1.216359e-06   468.325617   5.036471\n",
       "嘅                0.015173  3.927512e-06   151.298286   5.015457\n",
       "即係               0.014569  5.228434e-06   109.126105   4.761931\n",
       "喇                0.011954  1.792130e-06   261.236162   4.263261\n",
       "個                0.011616  6.182517e-07   735.830544   4.260456\n",
       "有                0.010742  6.631780e-07   634.364328   3.996522\n",
       "啲                0.010627  1.067155e-06   390.001220   3.922487\n",
       "囖                0.010860  1.547679e-06   274.795737   3.921209\n",
       "一                0.009017  7.837863e-07   450.525207   3.443651\n",
       "唔係               0.008248  6.415448e-07   503.512828   3.211352\n",
       "誒                0.008677  8.420489e-06    40.357965   2.993000\n",
       "又                0.007041  5.344590e-07   515.939635   2.837268\n",
       "話                0.007002  5.265057e-07   520.840354   2.816528\n",
       "去                0.008070  8.693631e-06    36.355415   2.794224\n",
       "到                0.006821  3.049769e-07   875.954960   2.782360\n",
       "喎                0.007009  8.837292e-07   310.597144   2.766509\n",
       "咗                0.006538  2.400715e-07  1066.533668   2.701735\n",
       "會                0.006656  7.434349e-07   350.634242   2.671996\n",
       "嗰啲               0.006493  6.213656e-07   409.208089   2.626787\n",
       "...                   ...           ...          ...        ...\n",
       "executive        0.000003  2.184325e-11  5657.401046   0.002272\n",
       "佣金               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "impression       0.000003  2.184325e-11  5657.401046   0.002272\n",
       "interviewer      0.000003  2.184325e-11  5657.401046   0.002272\n",
       "開朗               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "律師               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "foul             0.000003  2.184325e-11  5657.401046   0.002272\n",
       "knowledge        0.000003  2.184325e-11  5657.401046   0.002272\n",
       "心得               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "firsthon         0.000003  2.184325e-11  5657.401046   0.002272\n",
       "學術界              0.000003  2.184325e-11  5657.401046   0.002272\n",
       "十一點鐘             0.000003  2.184325e-11  5657.401046   0.002272\n",
       "mark             0.000003  2.184325e-11  5657.401046   0.002272\n",
       "人選               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "mistakes         0.000003  2.184325e-11  5657.401046   0.002272\n",
       "boyscott         0.000003  2.184325e-11  5657.401046   0.002272\n",
       "even             0.000003  2.184325e-11  5657.401046   0.002272\n",
       "回音               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "nature           0.000003  2.184325e-11  5657.401046   0.002272\n",
       "工作天              0.000003  2.184325e-11  5657.401046   0.002272\n",
       "濫                0.000003  2.184325e-11  5657.401046   0.002272\n",
       "dis              0.000003  2.184325e-11  5657.401046   0.002272\n",
       "customer         0.000003  2.184325e-11  5657.401046   0.002272\n",
       "層面               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "違約               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "居然               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "身在福中不知福          0.000003  2.184325e-11  5657.401046   0.002272\n",
       "趕住               0.000003  2.184325e-11  5657.401046   0.002272\n",
       "突然之間             0.000003  2.184325e-11  5657.401046   0.002272\n",
       "administrative   0.000003  2.184325e-11  5657.401046   0.002272\n",
       "\n",
       "[7070 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stopwords.sort_values('entropy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre: attribute_type must be 'sat_val', 'mean_prob', 'var_prob', 'entropy'\n",
    "def findRank(attribute_type, bool_ascending):\n",
    "    return df_stopwords.sort_values(\n",
    "        [attribute_type], ascending=bool_ascending ).reset_index().reset_index().set_index(\n",
    "        'word')[['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_sat_val = findRank('sat_val', True) # The higher the\n",
    "df_rank_mean_prob = findRank('mean_prob', False)\n",
    "df_rank_var_prob = findRank('var_prob', False)\n",
    "df_rank_entropy = findRank('entropy', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank = pd.DataFrame({\n",
    "    'sat_val_rank' : df_rank_sat_val['index'], \n",
    "    'mean_prob_rank' : df_rank_mean_prob['index'], \n",
    "    'var_prob_rank' : df_rank_var_prob['index'], \n",
    "    'entropy_rank' : df_rank_entropy['index'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank['weight'] = df_rank.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank.sort_values('weight', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output stop words\n",
    "# this list might need some further human cleaning\n",
    "df_rank['index'].head(200).to_csv(\n",
    "    r'/home/lun/Desktop/hkcancorpus_stopwords.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile HMM data from output in Part 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "df = df[['file_num', 'text', 'pos']].copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to check if string is an ascii\n",
    "def string_is_ascii(string):\n",
    "    try:\n",
    "        string.encode(encoding='ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# preprocessing function for BMES tagging of words\n",
    "#\n",
    "# It separates words to a list of characters\n",
    "# To preserve the ascii word,\n",
    "# find the first pointer ascii character position\n",
    "# find the last pointer ascii character position\n",
    "# concatenate ascii characters in the sublist, \n",
    "# removing empty strings and white spaces in between\n",
    "# parameter: word - word\n",
    "# returns: a list of tokens\n",
    "def tokenize_word(word):\n",
    "    char_list = list(word)\n",
    "#     print(\"Word is separated to : \" + str(char_list))\n",
    "\n",
    "    first_ascii_pos = []\n",
    "    last_ascii_pos = []\n",
    "    \n",
    "    ascii_flag = False\n",
    "    for i, c in enumerate(char_list):\n",
    "#         print(\"The current character is %s\" %c)\n",
    "        if string_is_ascii(c):           \n",
    "            if ascii_flag == False:\n",
    "#                 print(\"ascii set to true\")\n",
    "                first_ascii_pos.append(i)\n",
    "                ascii_flag = True\n",
    "            if i == len(char_list) - 1:\n",
    "                last_ascii_pos.append(len(char_list))\n",
    "        else:\n",
    "            if ascii_flag == True:\n",
    "#                 print(\"ascii set to false\")\n",
    "                last_ascii_pos.append(i)\n",
    "                ascii_flag = False\n",
    "        \n",
    "    if len(first_ascii_pos): # if array is not empty\n",
    "#         print(first_ascii_pos[::-1])\n",
    "#         print(last_ascii_pos[::-1])\n",
    "        for i, j in zip(first_ascii_pos[::-1], last_ascii_pos[::-1]):\n",
    "#             print(i, j)\n",
    "            char_list[i:j] = list(\n",
    "                filter(None, \"\".join(char_list[i:j]).split(\" \") ) )\n",
    "    return char_list\n",
    "\n",
    "    \n",
    "\n",
    "# function to tag words using the\n",
    "# BMES (begin, middle, end, single) tagging system\n",
    "# precondition: string must not be empty\n",
    "# returns list of separated words and corresponding\n",
    "# BMES tags\n",
    "def tagWord_BMES(word):\n",
    "    word_length = len(word)\n",
    "    assert(word_length)\n",
    "    bmes_list = []\n",
    "    \n",
    "    word_list = tokenize_word(word)\n",
    "    if len(word_list) == 1:\n",
    "        bmes_list.append(\"S\")\n",
    "    else:\n",
    "        for i, w in enumerate(word_list):\n",
    "            if i == 0:\n",
    "                bmes_list.append(\"B\")\n",
    "            elif i == len(word_list) - 1:\n",
    "                bmes_list.append(\"E\")\n",
    "            else:\n",
    "                bmes_list.append(\"M\")\n",
    "        \n",
    "    return bmes_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # test cases\n",
    "# print(tokenize_word('你office land過牆梯'))\n",
    "# print(tokenize_word('office'))\n",
    "# print(tokenize_word('你office'))\n",
    "# print(tokenize_word('hello過牆梯world'))\n",
    "# print(tokenize_word('hello過牆梯world過牆'))\n",
    "# print(tokenize_word('你過牆梯'))\n",
    "# print(tokenize_word('Hello  World'))\n",
    "\n",
    "# print(tagWord_BMES('你有過牆梯')) # BMMME\n",
    "# print(tagWord_BMES('office')) # S\n",
    "# print(tagWord_BMES('你office')) # BE\n",
    "# print(tagWord_BMES('office牆')) # BE\n",
    "# print(tagWord_BMES('Hello World')) # BE\n",
    "# print(tagWord_BMES('有有')) # BE\n",
    "# print(tagWord_BMES('有')) # S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# a helper to count total number of start instances\n",
    "def countTotalStartInstances(start_dict):\n",
    "    total = 0\n",
    "    for _, value in start_dict.items():\n",
    "        total += value\n",
    "    return total\n",
    "\n",
    "\n",
    "# create training algorithm to calculate \n",
    "# emission (BMES->word) and transition (BMES->BMES)\n",
    "# probabilities\n",
    "# returns: a tuple of prob_trans, prob_emit, and prob_start\n",
    "def trainingHMM_BMESTagging(text_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    \n",
    "    # for prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list in text_lists:\n",
    "        previous = '<s>'\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "        \n",
    "        for j, character in enumerate(line_list):\n",
    "#             print(\"The entry contains %s\" % character)\n",
    "            text_bmesTags_list = tagWord_BMES(character)\n",
    "            if j == 0:\n",
    "                start_tag = text_bmesTags_list[0]\n",
    "                if start_tag not in start:\n",
    "                    start[start_tag] = 0\n",
    "                start[start_tag] += 1\n",
    "        \n",
    "            for i, bmesTag in enumerate(text_bmesTags_list):\n",
    "                \n",
    "                transition_bigram = previous + \" \" + bmesTag\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if bmesTag not in context:\n",
    "                    context[bmesTag] = 0\n",
    "                context[bmesTag] += 1\n",
    "\n",
    "                bigram_emission = bmesTag + \" \" + character[i]\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "\n",
    "                previous = bmesTag\n",
    "            \n",
    "        bigram_transition = previous + \" </s>\"\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "\n",
    "    # output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for key, value in transition.items():\n",
    "        previous_tag, current_tag = key.split(\" \", maxsplit=1)\n",
    "        if previous_tag != '<s>' and current_tag != \"</s>\":\n",
    "            prob_trans[previous_tag][current_tag] = math.log2(float(value)/context[previous_tag])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    \n",
    "    for key, value in emission.items():\n",
    "        tag, word = key.split(\" \", maxsplit=1)\n",
    "#         print(\"Context contains %d instances\" % context[tag])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag])))\n",
    "        prob_emit[tag][word] = math.log2(float(value)/context[tag])\n",
    "    \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag, value in start.items():\n",
    "        prob_start[tag] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    prob_start[\"M\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    prob_start[\"E\"] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prob_trans1, prob_emit1, prob_start1 = trainingHMM_BMESTagging(df.text.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create training algorithm to calculate \n",
    "# emission (pos->word) and transition (pos-pos)\n",
    "# probabilities\n",
    "def trainingHMM_POSTagging(text_lists, posTags_lists):\n",
    "    emission = {}\n",
    "    transition = {}\n",
    "    context = {} \n",
    "    start = {}\n",
    "    char_state = {}\n",
    "    \n",
    "    # for char_state_tab.*, prob_*.* files in jieba\n",
    "    prob_trans = collections.defaultdict(dict)\n",
    "    prob_emit = collections.defaultdict(dict)\n",
    "    prob_start = {}\n",
    "    \n",
    "    pos_tagset1 = [pos_tag for posTag_list in posTags_lists for pos_tag in posTag_list]\n",
    "    pos_tagset2 = ['ag', 'a', 'ad', 'an', 'bg', 'b', 'c', 'dg', 'd', \n",
    "        'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'mg', 'm', 'ng', 'n', \n",
    "        'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'qg', 'q', 'rg', 'r', \n",
    "        's', 'tg', 't', 'ug', 'u', 'vg', 'v', 'vd', 'vn', 'w', 'x', \n",
    "        'yg', 'y', 'z'] # official HKCanCor tagset\n",
    "    unique_pos_tagset = set(pos_tagset1 + pos_tagset2)\n",
    "    bmes_tagset = {'B', 'M', 'E', 'S'}\n",
    "    \n",
    "    # this is for the training part\n",
    "    for line_list, linePosTags_list in zip(text_lists, posTags_lists):\n",
    "        previous = ('<s>') # start sentence tag\n",
    "        if previous not in context:\n",
    "            context[previous] = 0\n",
    "        context[previous] += 1\n",
    "       \n",
    "        for j, (character, pos_tag) in enumerate(\n",
    "            zip(line_list, linePosTags_list) ):\n",
    "#             print(\"The entry contains %s with tag %s \" % (character, pos_tag) )\n",
    "            character_bmesTags_list = tagWord_BMES(character)\n",
    "            character_token_list = tokenize_word(character)\n",
    "            \n",
    "            # build up start dictionary\n",
    "            if j == 0:\n",
    "                start_tag = character_bmesTags_list[0]\n",
    "                if (start_tag, pos_tag) not in start:\n",
    "                    start[(start_tag, pos_tag)] = 0\n",
    "                start[(start_tag, pos_tag)] += 1\n",
    "        \n",
    "            # build up transition, emission dictionaries\n",
    "            for i, (token, bmesTag) in enumerate(\n",
    "                zip(character_token_list, character_bmesTags_list)):\n",
    "                \n",
    "                tag_pair = (bmesTag, pos_tag)\n",
    "                \n",
    "                transition_bigram = (previous, tag_pair)\n",
    "                if transition_bigram not in transition:\n",
    "                    transition[transition_bigram] = 0\n",
    "                transition[transition_bigram] += 1\n",
    "\n",
    "                if tag_pair not in context:\n",
    "                    context[tag_pair] = 0\n",
    "                context[tag_pair] += 1\n",
    "\n",
    "                bigram_emission = (tag_pair, token)\n",
    "                if bigram_emission not in emission:\n",
    "                    emission[bigram_emission] = 0\n",
    "                emission[bigram_emission] += 1\n",
    "                \n",
    "                if token not in char_state:\n",
    "                    char_state[token] = []\n",
    "                char_state[token].append(tag_pair)\n",
    "                    \n",
    "                \n",
    "                previous = tag_pair\n",
    "            \n",
    "        bigram_transition = (previous, \"</s>\")\n",
    "        if bigram_transition not in transition:\n",
    "            transition[bigram_transition] = 0\n",
    "        transition[bigram_transition] += 1\n",
    "        \n",
    "    \n",
    "# output transition, emission and start probabilities\n",
    "#     print(context)\n",
    "    for (previous_tag_pair, current_tag_pair), value in transition.items():\n",
    "        if previous_tag_pair != ('<s>') and current_tag_pair != (\"</s>\"):\n",
    "            prob_trans[previous_tag_pair][current_tag_pair] = math.log2(float(value)/context[previous_tag_pair])\n",
    "#         print(\"Transition probability of %s is %.15f\" % (key, math.log2(float(value)/context[previous_tag]) ) )        \n",
    "#     print(\"\\n\\n\\n\")\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_trans:\n",
    "                prob_trans[(bmes_tag, pos_tag)] = {}\n",
    "                \n",
    "    \n",
    "    for (token, tag_pair_list) in char_state.items():\n",
    "        char_state[token] = tuple(set(tag_pair_list)) # only keep unique tag sets\n",
    "        \n",
    "    \n",
    "    for (tag_pair, word), value in emission.items():\n",
    "#         print(\"Context contains %d instances\" % context[tag_pair])\n",
    "#         print(\"tag is %s, which emits %s, with emission probability of %.15f\\n\" % (\n",
    "#             tag, word,  math.log2(float(value)/context[tag_pair])))\n",
    "        prob_emit[tag_pair][word] = math.log2(float(value)/context[tag_pair])\n",
    "    \n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_emit:\n",
    "                prob_emit[(bmes_tag, pos_tag)] = {}\n",
    "\n",
    "                \n",
    "#     print(\"Start dict contains\" + str(start))\n",
    "    for tag_pair, value in start.items():\n",
    "        prob_start[tag_pair] = math.log2(float(value)/countTotalStartInstances(start))\n",
    "    for bmes_tag in bmes_tagset: # do this for empty tag pairs \n",
    "        for pos_tag in unique_pos_tagset:\n",
    "            if (bmes_tag, pos_tag) not in prob_start:\n",
    "                prob_start[(bmes_tag, pos_tag)] = -3.14e100 # minimum float value defined in jieba (MIN_FLOAT)\n",
    "    \n",
    "    return dict(prob_trans), dict(prob_emit), prob_start, char_state"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prob_trans2, prob_emit2, prob_start2, char_state2 = trainingHMM_POSTagging(df.text.tolist(), df.pos.tolist())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# char_state2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# s4, s5, s6, s7 = trainingHMM_POSTagging([['重', '有得', '搞']], [['d', 'vu', 'v']])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Output prob_trans, prob_emit, prob_start"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def outputDictionary(filename, prob_dict):\n",
    "    with open(OUTPUT_PATH + filename, 'w', encoding='utf-8') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(\"P=\", end='')\n",
    "            pprint(prob_dict) \n",
    "    assert(f.closed)\n",
    "\n",
    "def pickleDictionary(filename, prob_dict):\n",
    "    with open(OUTPUT_PATH + filename, 'wb') as f:\n",
    "        # jieba uses protocol 0 encoding for its pickle files\n",
    "        pickle.dump(prob_dict, f, protocol=0)\n",
    "    assert(f.closed)\n",
    "    \n",
    "def depickleDictionary(filename):\n",
    "    with open(OUTPUT_PATH + filename, 'rb') as f:\n",
    "        prob_dict = pickle.load(f, encoding='utf-8')\n",
    "    assert(f.closed)\n",
    "    return prob_dict\n",
    "    \n",
    "outputDictionary(\"posseg/prob_trans.py\", prob_trans2)\n",
    "outputDictionary(\"posseg/prob_emit.py\", prob_emit2)\n",
    "outputDictionary(\"posseg/prob_start.py\", prob_start2)\n",
    "outputDictionary(\"posseg/char_state_tab.py\", char_state2)\n",
    "\n",
    "pickleDictionary(\"posseg/prob_trans.p\", prob_trans2)\n",
    "pickleDictionary(\"posseg/prob_emit.p\", prob_emit2)\n",
    "pickleDictionary(\"posseg/prob_start.p\", prob_start2)\n",
    "pickleDictionary(\"posseg/char_state_tab.p\", char_state2)\n",
    "\n",
    "outputDictionary(\"finalseg/prob_trans.py\", prob_trans1)\n",
    "outputDictionary(\"finalseg/prob_emit.py\", prob_emit1)\n",
    "outputDictionary(\"finalseg/prob_start.py\", prob_start1)\n",
    "\n",
    "pickleDictionary(\"finalseg/prob_trans.p\", prob_trans1)\n",
    "pickleDictionary(\"finalseg/prob_emit.p\", prob_emit1)\n",
    "pickleDictionary(\"finalseg/prob_start.p\", prob_start1)\n",
    "\n",
    "\n",
    "# s1 = depickleDictionary(\"finalseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"finalseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"finalseg/prob_start.p\")\n",
    "\n",
    "# s1 = depickleDictionary(\"posseg/prob_trans.p\")\n",
    "# s2 = depickleDictionary(\"posseg/prob_emit.p\")\n",
    "# s3 = depickleDictionary(\"posseg/prob_start.p\")\n",
    "# s4 = depickleDictionary(\"posseg/char_state_tab.p\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Statistical Data from HKCanCor CHAT files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "corpus = pc.hkcancor()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "words = corpus.search(pos='.+')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def extract_POSInfoFromPyCantonese(words):\n",
    "    word_list = []\n",
    "    for i in range(len(words)):\n",
    "        # we need the word and POS tags only\n",
    "        word_list.append(\" \".join(words[i][0:2]))\n",
    "    return word_list\n",
    "\n",
    "full_list = extract_POSInfoFromPyCantonese(words)\n",
    "\n",
    "print(len(full_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put everything in Pandas\n",
    "# This is not necessary, but \n",
    "# it shows the layouts neatly\n",
    "df_full = pd.DataFrame(full_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full.columns = ['tokens']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full = pd.DataFrame(\n",
    "    df_full.tokens.str.split(' ', 1).tolist(), \n",
    "    columns=['word', 'pos'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full['pos'] = df_full['pos'].str.lower()\n",
    "df_full['word'] = df_full['word'].apply(replaceCantoneseCharacters)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full = df_full.groupby(['word','pos'], sort=False).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# reorder the columns according to Jieba Dictionary layout\n",
    "df_full = df_full[['word', 'count', 'pos']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove all ?.\"'! etc\n",
    "df_full.drop(df_full.index[:7], inplace=True)\n",
    "df_full.drop(df_full.index[2], inplace=True)\n",
    "df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_nouns = df_full[df_full.pos.str.contains('^n|[^va]n', regex=True)]\n",
    "df_nouns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_others = df_full[~df_full.isin(df_nouns)].dropna()\n",
    "df_others"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_nouns.word.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/nouns.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_others.word.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/others.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_full.to_csv(\n",
    "    r'/home/lun/csrp/dictionaries/hkcantonesedict.txt', \n",
    "    sep=' ', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
